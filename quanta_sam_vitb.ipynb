{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d8f3c3",
   "metadata": {},
   "source": [
    "## 1. Load SAM ViT-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42222a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allank24/Programming/HQCNN/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/allank24/Programming/HQCNN/.venv/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: '/home/allank24/Programming/HQCNN/.venv/lib/python3.12/site-packages/torchvision/image.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SamModel(\n",
       "  (shared_image_embedding): SamPositionalEmbedding()\n",
       "  (vision_encoder): SamVisionEncoder(\n",
       "    (patch_embed): SamPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x SamVisionLayer(\n",
       "        (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): SamVisionAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): SamMLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): SamVisionNeck(\n",
       "      (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): SamPromptEncoder(\n",
       "    (shared_embedding): SamPositionalEmbedding()\n",
       "    (mask_embed): SamMaskEmbedding(\n",
       "      (activation): GELUActivation()\n",
       "      (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (layer_norm1): SamLayerNorm()\n",
       "      (layer_norm2): SamLayerNorm()\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "    (point_embed): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): SamMaskDecoder(\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (transformer): SamTwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x SamTwoWayAttentionBlock(\n",
       "          (self_attn): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SamMLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): SamAttention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): SamAttention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (upscale_layer_norm): SamLayerNorm()\n",
       "    (activation): GELU(approximate='none')\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x SamFeedForward(\n",
       "        (activation): ReLU()\n",
       "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): SamFeedForward(\n",
       "      (activation): ReLU()\n",
       "      (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "      (layers): ModuleList(\n",
       "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_path = \"/home/allank24/Programming/HQCNN/Segmentation/sam-vit-base\"\n",
    "\n",
    "model = SamModel.from_pretrained(model_path).to(\"cuda\")\n",
    "processor = SamProcessor.from_pretrained(model_path)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ba8781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vision_encoder.layers.0.attn.qkv\n",
      "vision_encoder.layers.0.attn.proj\n",
      "vision_encoder.layers.0.mlp.lin1\n",
      "vision_encoder.layers.0.mlp.lin2\n",
      "vision_encoder.layers.1.attn.qkv\n",
      "vision_encoder.layers.1.attn.proj\n",
      "vision_encoder.layers.1.mlp.lin1\n",
      "vision_encoder.layers.1.mlp.lin2\n",
      "vision_encoder.layers.2.attn.qkv\n",
      "vision_encoder.layers.2.attn.proj\n",
      "vision_encoder.layers.2.mlp.lin1\n",
      "vision_encoder.layers.2.mlp.lin2\n",
      "vision_encoder.layers.3.attn.qkv\n",
      "vision_encoder.layers.3.attn.proj\n",
      "vision_encoder.layers.3.mlp.lin1\n",
      "vision_encoder.layers.3.mlp.lin2\n",
      "vision_encoder.layers.4.attn.qkv\n",
      "vision_encoder.layers.4.attn.proj\n",
      "vision_encoder.layers.4.mlp.lin1\n",
      "vision_encoder.layers.4.mlp.lin2\n",
      "vision_encoder.layers.5.attn.qkv\n",
      "vision_encoder.layers.5.attn.proj\n",
      "vision_encoder.layers.5.mlp.lin1\n",
      "vision_encoder.layers.5.mlp.lin2\n",
      "vision_encoder.layers.6.attn.qkv\n",
      "vision_encoder.layers.6.attn.proj\n",
      "vision_encoder.layers.6.mlp.lin1\n",
      "vision_encoder.layers.6.mlp.lin2\n",
      "vision_encoder.layers.7.attn.qkv\n",
      "vision_encoder.layers.7.attn.proj\n",
      "vision_encoder.layers.7.mlp.lin1\n",
      "vision_encoder.layers.7.mlp.lin2\n",
      "vision_encoder.layers.8.attn.qkv\n",
      "vision_encoder.layers.8.attn.proj\n",
      "vision_encoder.layers.8.mlp.lin1\n",
      "vision_encoder.layers.8.mlp.lin2\n",
      "vision_encoder.layers.9.attn.qkv\n",
      "vision_encoder.layers.9.attn.proj\n",
      "vision_encoder.layers.9.mlp.lin1\n",
      "vision_encoder.layers.9.mlp.lin2\n",
      "vision_encoder.layers.10.attn.qkv\n",
      "vision_encoder.layers.10.attn.proj\n",
      "vision_encoder.layers.10.mlp.lin1\n",
      "vision_encoder.layers.10.mlp.lin2\n",
      "vision_encoder.layers.11.attn.qkv\n",
      "vision_encoder.layers.11.attn.proj\n",
      "vision_encoder.layers.11.mlp.lin1\n",
      "vision_encoder.layers.11.mlp.lin2\n",
      "mask_decoder.transformer.layers.0.self_attn.q_proj\n",
      "mask_decoder.transformer.layers.0.self_attn.k_proj\n",
      "mask_decoder.transformer.layers.0.self_attn.v_proj\n",
      "mask_decoder.transformer.layers.0.self_attn.out_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj\n",
      "mask_decoder.transformer.layers.0.mlp.lin1\n",
      "mask_decoder.transformer.layers.0.mlp.lin2\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj\n",
      "mask_decoder.transformer.layers.1.self_attn.q_proj\n",
      "mask_decoder.transformer.layers.1.self_attn.k_proj\n",
      "mask_decoder.transformer.layers.1.self_attn.v_proj\n",
      "mask_decoder.transformer.layers.1.self_attn.out_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj\n",
      "mask_decoder.transformer.layers.1.mlp.lin1\n",
      "mask_decoder.transformer.layers.1.mlp.lin2\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj\n",
      "mask_decoder.transformer.final_attn_token_to_image.q_proj\n",
      "mask_decoder.transformer.final_attn_token_to_image.k_proj\n",
      "mask_decoder.transformer.final_attn_token_to_image.v_proj\n",
      "mask_decoder.transformer.final_attn_token_to_image.out_proj\n",
      "mask_decoder.output_hypernetworks_mlps.0.proj_in\n",
      "mask_decoder.output_hypernetworks_mlps.0.proj_out\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.0\n",
      "mask_decoder.output_hypernetworks_mlps.1.proj_in\n",
      "mask_decoder.output_hypernetworks_mlps.1.proj_out\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.0\n",
      "mask_decoder.output_hypernetworks_mlps.2.proj_in\n",
      "mask_decoder.output_hypernetworks_mlps.2.proj_out\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.0\n",
      "mask_decoder.output_hypernetworks_mlps.3.proj_in\n",
      "mask_decoder.output_hypernetworks_mlps.3.proj_out\n",
      "mask_decoder.output_hypernetworks_mlps.3.layers.0\n",
      "mask_decoder.iou_prediction_head.proj_in\n",
      "mask_decoder.iou_prediction_head.proj_out\n",
      "mask_decoder.iou_prediction_head.layers.0\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module,torch.nn.Linear):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d918203d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = [\n",
    "    \"vision_encoder.layers.0.attn.qkv\",\n",
    "    \"vision_encoder.layers.1.attn.qkv\",\n",
    "    \"vision_encoder.layers.2.attn.qkv\",\n",
    "    \"vision_encoder.layers.3.attn.qkv\",\n",
    "    \"vision_encoder.layers.4.attn.qkv\",\n",
    "    \"vision_encoder.layers.5.attn.qkv\",\n",
    "    \"vision_encoder.layers.6.attn.qkv\",\n",
    "    \"vision_encoder.layers.7.attn.qkv\",\n",
    "    \"vision_encoder.layers.8.attn.qkv\",\n",
    "    \"vision_encoder.layers.9.attn.qkv\",\n",
    "    \"vision_encoder.layers.10.attn.qkv\",\n",
    "    \"vision_encoder.layers.11.attn.qkv\",\n",
    "    # Mask Decoder\n",
    "    \"mask_decoder.transformer.layers.0.self_attn.q_proj\",\n",
    "    # \"mask_decoder.transformer.layers.0.self_attn.k_proj\",\n",
    "    \"mask_decoder.transformer.layers.0.self_attn.v_proj\",\n",
    "    'mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj',\n",
    "    'mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj',\n",
    "    'mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj',\n",
    "    'mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj',\n",
    "    'mask_decoder.transformer.layers.1.self_attn.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.1.self_attn.k_proj',\n",
    "    'mask_decoder.transformer.layers.1.self_attn.v_proj',\n",
    "    'mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj',\n",
    "    'mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj',\n",
    "    'mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj',\n",
    "    'mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj',\n",
    "    'mask_decoder.transformer.final_attn_token_to_image.q_proj',\n",
    "    # 'mask_decoder.transformer.final_attn_token_to_image.k_proj',\n",
    "    'mask_decoder.transformer.final_attn_token_to_image.v_proj'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dda7636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allank24/Programming/HQCNN/Segmentation/quanta/quanta/quanta/tuners/quanta.py:293: UserWarning: per_dim_features2=[12, 8, 8] does not match out_features=2304, this should work but may result in downgraded performance or additional cost. Please make sure this is intended.\n",
      "  warnings.warn(\n",
      "/home/allank24/Programming/HQCNN/Segmentation/quanta/quanta/quanta/tuners/quanta.py:290: UserWarning: per_dim_features=[12, 8, 8] does not match in_features=256, this should work but may result in downgraded performance or additional cost. Please make sure this is intended.\n",
      "  warnings.warn(\n",
      "/home/allank24/Programming/HQCNN/Segmentation/quanta/quanta/quanta/tuners/quanta.py:293: UserWarning: per_dim_features2=[12, 8, 8] does not match out_features=256, this should work but may result in downgraded performance or additional cost. Please make sure this is intended.\n",
      "  warnings.warn(\n",
      "/home/allank24/Programming/HQCNN/Segmentation/quanta/quanta/quanta/tuners/quanta.py:293: UserWarning: per_dim_features2=[12, 8, 8] does not match out_features=128, this should work but may result in downgraded performance or additional cost. Please make sure this is intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "QuanTAModel(\n",
       "  (model): SamModel(\n",
       "    (shared_image_embedding): SamPositionalEmbedding()\n",
       "    (vision_encoder): SamVisionEncoder(\n",
       "      (patch_embed): SamPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x SamVisionLayer(\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): SamVisionAttention(\n",
       "            (qkv): Linear(\n",
       "              in_features=768, out_features=2304, bias=True\n",
       "              (merged): MergeBuffer()\n",
       "              (frozen_merged): MergeBuffer()\n",
       "              (quanta_weights): ParameterDict(\n",
       "                  (-1 -2): Parameter containing: [torch.cuda.FloatTensor of size 8x8x8x8 (cuda:0)]\n",
       "                  (-1 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                  (-2 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "              )\n",
       "              (quanta_weights2): BufferDict()\n",
       "            )\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SamMLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELUActivation()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): SamVisionNeck(\n",
       "        (conv1): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (layer_norm1): SamLayerNorm()\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (layer_norm2): SamLayerNorm()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): SamPromptEncoder(\n",
       "      (shared_embedding): SamPositionalEmbedding()\n",
       "      (mask_embed): SamMaskEmbedding(\n",
       "        (activation): GELUActivation()\n",
       "        (conv1): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (conv2): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (conv3): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (layer_norm1): SamLayerNorm()\n",
       "        (layer_norm2): SamLayerNorm()\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "      (point_embed): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): SamMaskDecoder(\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (transformer): SamTwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x SamTwoWayAttentionBlock(\n",
       "            (self_attn): SamAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=256, out_features=256, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.cuda.FloatTensor of size 8x8x8x8 (cuda:0)]\n",
       "                    (-1 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                    (-2 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=256, out_features=256, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.cuda.FloatTensor of size 8x8x8x8 (cuda:0)]\n",
       "                    (-1 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                    (-2 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): SamAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=256, out_features=128, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.cuda.FloatTensor of size 8x8x8x8 (cuda:0)]\n",
       "                    (-1 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                    (-2 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=256, out_features=128, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.cuda.FloatTensor of size 8x8x8x8 (cuda:0)]\n",
       "                    (-1 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                    (-2 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "            (mlp): SamMLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm3): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "            (layer_norm4): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): SamAttention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=256, out_features=128, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.cuda.FloatTensor of size 8x8x8x8 (cuda:0)]\n",
       "                    (-1 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                    (-2 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=256, out_features=128, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.cuda.FloatTensor of size 8x8x8x8 (cuda:0)]\n",
       "                    (-1 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                    (-2 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): SamAttention(\n",
       "          (q_proj): Linear(\n",
       "            in_features=256, out_features=128, bias=True\n",
       "            (merged): MergeBuffer()\n",
       "            (frozen_merged): MergeBuffer()\n",
       "            (quanta_weights): ParameterDict(\n",
       "                (-1 -2): Parameter containing: [torch.cuda.FloatTensor of size 8x8x8x8 (cuda:0)]\n",
       "                (-1 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                (-2 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "            )\n",
       "            (quanta_weights2): BufferDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(\n",
       "            in_features=256, out_features=128, bias=True\n",
       "            (merged): MergeBuffer()\n",
       "            (frozen_merged): MergeBuffer()\n",
       "            (quanta_weights): ParameterDict(\n",
       "                (-1 -2): Parameter containing: [torch.cuda.FloatTensor of size 8x8x8x8 (cuda:0)]\n",
       "                (-1 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "                (-2 -3): Parameter containing: [torch.cuda.FloatTensor of size 12x8x12x8 (cuda:0)]\n",
       "            )\n",
       "            (quanta_weights2): BufferDict()\n",
       "          )\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (layer_norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (upscale_conv1): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (upscale_conv2): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (upscale_layer_norm): SamLayerNorm()\n",
       "      (activation): GELU(approximate='none')\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x SamFeedForward(\n",
       "          (activation): ReLU()\n",
       "          (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (proj_out): Linear(in_features=256, out_features=32, bias=True)\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): SamFeedForward(\n",
       "        (activation): ReLU()\n",
       "        (proj_in): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (proj_out): Linear(in_features=256, out_features=4, bias=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quanta import QuanTAConfig, QuanTAModel\n",
    "\n",
    "\n",
    "qconfig = QuanTAConfig(\n",
    "    d=3,\n",
    "    per_dim_features=[12, 8, 8],\n",
    "    target_modules=target_modules,\n",
    "    merge_weights=False,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "quanta_model = QuanTAModel(qconfig, model)\n",
    "quanta_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4857b4",
   "metadata": {},
   "source": [
    "## Load a model in a different way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d44c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allank24/Programming/HQCNN/.venv/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: '/home/allank24/Programming/HQCNN/.venv/lib/python3.12/site-packages/torchvision/image.so: undefined symbol: _ZN3c108ListType3getERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEENS_4Type24SingletonOrSharedTypePtrIS9_EE'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/home/allank24/Programming/HQCNN/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/allank24/Programming/HQCNN/.venv/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/allank24/Programming/HQCNN/.venv/lib/python3.12/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/allank24/Programming/HQCNN/Segmentation/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_5m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/allank24/Programming/HQCNN/Segmentation/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_11m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/allank24/Programming/HQCNN/Segmentation/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/allank24/Programming/HQCNN/Segmentation/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_384 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n",
      "/home/allank24/Programming/HQCNN/Segmentation/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_512 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  return register_model(fn_wrapper)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(3, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-2): 3 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=3, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import types, torch\n",
    "from models.sam import SamPredictor, sam_model_registry\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Create the object with initial attributes\n",
    "args = types.SimpleNamespace(\n",
    "    sam_ckpt='/home/allank24/Programming/HQCNN/Segmentation/finetune-SAM/sam_vit_b_weights/sam_vit_b_01ec64.pth',\n",
    "    net='sam',\n",
    "    arch='vit_b',\n",
    "    baseline='unet',\n",
    "    dataset_name='MRI-Prostate',\n",
    "    img_folder='./datasets/',\n",
    "    mask_folder='./datasets/',\n",
    "    train_img_list='./datasets/train.csv',\n",
    "    val_img_list='./datasets/val.csv',\n",
    "    test_img_list='./datasets/train.csv',\n",
    "    targets='combine_all',\n",
    "    finetune_type='lora',\n",
    "    normalize_type='sam',\n",
    "    dir_checkpoint='checkpoints',\n",
    "    num_cls=2,\n",
    "    epochs=200,\n",
    "    type='map',\n",
    "    vis=None,\n",
    "    reverse=False,\n",
    "    pretrain=False,\n",
    "    val_freq=100,\n",
    "    gpu=True,\n",
    "    gpu_device=0,\n",
    "    sim_gpu=0,\n",
    "    epoch_ini=1,\n",
    "    image_size=1024,\n",
    "    out_size=256,\n",
    "    patch_size=2,\n",
    "    dim=512,\n",
    "    depth=64,\n",
    "    heads=16,\n",
    "    mlp_dim=1024,\n",
    "    w=4,\n",
    "    b=4,\n",
    "    s=True,\n",
    "    if_warmup=False,\n",
    "    warmup_period=200,\n",
    "    lr=1e-3,\n",
    "    uinch=1,\n",
    "    imp_lr=3e-4,\n",
    "    weights=0,\n",
    "    base_weights=0,\n",
    "    sim_weights=0,\n",
    "    distributed='none',\n",
    "    dataset='isic',\n",
    "    thd=False,\n",
    "    chunk=96,\n",
    "    num_sample=4,\n",
    "    roi_size=96,\n",
    "    if_update_encoder=False,\n",
    "    if_encoder_adapter=False,\n",
    "    encoder_adapter_depths=[0, 1, 10, 11],\n",
    "    if_mask_decoder_adapter=False,\n",
    "    decoder_adapt_depth=2,\n",
    "    if_encoder_lora_layer=False,\n",
    "    if_decoder_lora_layer=False,\n",
    "    encoder_lora_layer=[0, 1, 10, 11],\n",
    "    if_split_encoder_gpus=False,\n",
    "    devices=device,\n",
    "    gpu_fractions=[0.5, 0.5],\n",
    "    evl_chunk=None,\n",
    "    tau=1.0\n",
    ")\n",
    "\n",
    "model = sam_model_registry[\"vit_b\"](args,checkpoint=args.sam_ckpt,num_classes=2)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34b8f83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_encoder.blocks.0.attn.qkv\n",
      "image_encoder.blocks.0.attn.proj\n",
      "image_encoder.blocks.0.mlp.lin1\n",
      "image_encoder.blocks.0.mlp.lin2\n",
      "image_encoder.blocks.1.attn.qkv\n",
      "image_encoder.blocks.1.attn.proj\n",
      "image_encoder.blocks.1.mlp.lin1\n",
      "image_encoder.blocks.1.mlp.lin2\n",
      "image_encoder.blocks.2.attn.qkv\n",
      "image_encoder.blocks.2.attn.proj\n",
      "image_encoder.blocks.2.mlp.lin1\n",
      "image_encoder.blocks.2.mlp.lin2\n",
      "image_encoder.blocks.3.attn.qkv\n",
      "image_encoder.blocks.3.attn.proj\n",
      "image_encoder.blocks.3.mlp.lin1\n",
      "image_encoder.blocks.3.mlp.lin2\n",
      "image_encoder.blocks.4.attn.qkv\n",
      "image_encoder.blocks.4.attn.proj\n",
      "image_encoder.blocks.4.mlp.lin1\n",
      "image_encoder.blocks.4.mlp.lin2\n",
      "image_encoder.blocks.5.attn.qkv\n",
      "image_encoder.blocks.5.attn.proj\n",
      "image_encoder.blocks.5.mlp.lin1\n",
      "image_encoder.blocks.5.mlp.lin2\n",
      "image_encoder.blocks.6.attn.qkv\n",
      "image_encoder.blocks.6.attn.proj\n",
      "image_encoder.blocks.6.mlp.lin1\n",
      "image_encoder.blocks.6.mlp.lin2\n",
      "image_encoder.blocks.7.attn.qkv\n",
      "image_encoder.blocks.7.attn.proj\n",
      "image_encoder.blocks.7.mlp.lin1\n",
      "image_encoder.blocks.7.mlp.lin2\n",
      "image_encoder.blocks.8.attn.qkv\n",
      "image_encoder.blocks.8.attn.proj\n",
      "image_encoder.blocks.8.mlp.lin1\n",
      "image_encoder.blocks.8.mlp.lin2\n",
      "image_encoder.blocks.9.attn.qkv\n",
      "image_encoder.blocks.9.attn.proj\n",
      "image_encoder.blocks.9.mlp.lin1\n",
      "image_encoder.blocks.9.mlp.lin2\n",
      "image_encoder.blocks.10.attn.qkv\n",
      "image_encoder.blocks.10.attn.proj\n",
      "image_encoder.blocks.10.mlp.lin1\n",
      "image_encoder.blocks.10.mlp.lin2\n",
      "image_encoder.blocks.11.attn.qkv\n",
      "image_encoder.blocks.11.attn.proj\n",
      "image_encoder.blocks.11.mlp.lin1\n",
      "image_encoder.blocks.11.mlp.lin2\n",
      "mask_decoder.transformer.layers.0.self_attn.q_proj\n",
      "mask_decoder.transformer.layers.0.self_attn.k_proj\n",
      "mask_decoder.transformer.layers.0.self_attn.v_proj\n",
      "mask_decoder.transformer.layers.0.self_attn.out_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj\n",
      "mask_decoder.transformer.layers.0.mlp.lin1\n",
      "mask_decoder.transformer.layers.0.mlp.lin2\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj\n",
      "mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj\n",
      "mask_decoder.transformer.layers.1.self_attn.q_proj\n",
      "mask_decoder.transformer.layers.1.self_attn.k_proj\n",
      "mask_decoder.transformer.layers.1.self_attn.v_proj\n",
      "mask_decoder.transformer.layers.1.self_attn.out_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj\n",
      "mask_decoder.transformer.layers.1.mlp.lin1\n",
      "mask_decoder.transformer.layers.1.mlp.lin2\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj\n",
      "mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj\n",
      "mask_decoder.transformer.final_attn_token_to_image.q_proj\n",
      "mask_decoder.transformer.final_attn_token_to_image.k_proj\n",
      "mask_decoder.transformer.final_attn_token_to_image.v_proj\n",
      "mask_decoder.transformer.final_attn_token_to_image.out_proj\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.0\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.1\n",
      "mask_decoder.output_hypernetworks_mlps.0.layers.2\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.0\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.1\n",
      "mask_decoder.output_hypernetworks_mlps.1.layers.2\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.0\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.1\n",
      "mask_decoder.output_hypernetworks_mlps.2.layers.2\n",
      "mask_decoder.iou_prediction_head.layers.0\n",
      "mask_decoder.iou_prediction_head.layers.1\n",
      "mask_decoder.iou_prediction_head.layers.2\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if isinstance(module,torch.nn.Linear):\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b1e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = [\n",
    "    \"image_encoder.blocks.0.attn.qkv\",\n",
    "    \"image_encoder.blocks.1.attn.qkv\",\n",
    "    \"image_encoder.blocks.2.attn.qkv\",\n",
    "    \"image_encoder.blocks.3.attn.qkv\",\n",
    "    \"image_encoder.blocks.4.attn.qkv\",\n",
    "    \"image_encoder.blocks.5.attn.qkv\",\n",
    "    \"image_encoder.blocks.6.attn.qkv\",\n",
    "    \"image_encoder.blocks.7.attn.qkv\",\n",
    "    \"image_encoder.blocks.8.attn.qkv\",\n",
    "    \"image_encoder.blocks.9.attn.qkv\",\n",
    "    \"image_encoder.blocks.10.attn.qkv\",\n",
    "    \"image_encoder.blocks.11.attn.qkv\",\n",
    "    # Mask Decoder\n",
    "    \"mask_decoder.transformer.layers.0.self_attn.q_proj\",\n",
    "    # \"mask_decoder.transformer.layers.0.self_attn.k_proj\",\n",
    "    \"mask_decoder.transformer.layers.0.self_attn.v_proj\",\n",
    "    'mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj',\n",
    "    'mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj',\n",
    "    'mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj',\n",
    "    'mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj',\n",
    "    'mask_decoder.transformer.layers.1.self_attn.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.1.self_attn.k_proj',\n",
    "    'mask_decoder.transformer.layers.1.self_attn.v_proj',\n",
    "    'mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj',\n",
    "    'mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj',\n",
    "    'mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj',\n",
    "    # 'mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj',\n",
    "    'mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj',\n",
    "    'mask_decoder.transformer.final_attn_token_to_image.q_proj',\n",
    "    # 'mask_decoder.transformer.final_attn_token_to_image.k_proj',\n",
    "    'mask_decoder.transformer.final_attn_token_to_image.v_proj'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "764de5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 93,595,151\n",
      "Trainable parameters: 93,595,151\n",
      "Frozen parameters: 0\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "# Example\n",
    "total, trainable = count_parameters(model)\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Frozen parameters: {total - trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "349a8f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuanTAModel(\n",
       "  (model): Sam(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(\n",
       "              in_features=768, out_features=2304, bias=True\n",
       "              (merged): MergeBuffer()\n",
       "              (frozen_merged): MergeBuffer()\n",
       "              (quanta_weights): ParameterDict(\n",
       "                  (-1 -2): Parameter containing: [torch.FloatTensor of size 8x8x8x8]\n",
       "                  (-1 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                  (-2 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "              )\n",
       "              (quanta_weights2): BufferDict()\n",
       "            )\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): Attention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=256, out_features=256, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.FloatTensor of size 8x8x8x8]\n",
       "                    (-1 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                    (-2 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=256, out_features=256, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.FloatTensor of size 8x8x8x8]\n",
       "                    (-1 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                    (-2 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): Attention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=256, out_features=128, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.FloatTensor of size 8x8x8x8]\n",
       "                    (-1 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                    (-2 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=256, out_features=128, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.FloatTensor of size 8x8x8x8]\n",
       "                    (-1 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                    (-2 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): Attention(\n",
       "              (q_proj): Linear(\n",
       "                in_features=256, out_features=128, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.FloatTensor of size 8x8x8x8]\n",
       "                    (-1 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                    (-2 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(\n",
       "                in_features=256, out_features=128, bias=True\n",
       "                (merged): MergeBuffer()\n",
       "                (frozen_merged): MergeBuffer()\n",
       "                (quanta_weights): ParameterDict(\n",
       "                    (-1 -2): Parameter containing: [torch.FloatTensor of size 8x8x8x8]\n",
       "                    (-1 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                    (-2 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                )\n",
       "                (quanta_weights2): BufferDict()\n",
       "              )\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): Attention(\n",
       "          (q_proj): Linear(\n",
       "            in_features=256, out_features=128, bias=True\n",
       "            (merged): MergeBuffer()\n",
       "            (frozen_merged): MergeBuffer()\n",
       "            (quanta_weights): ParameterDict(\n",
       "                (-1 -2): Parameter containing: [torch.FloatTensor of size 8x8x8x8]\n",
       "                (-1 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                (-2 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "            )\n",
       "            (quanta_weights2): BufferDict()\n",
       "          )\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(\n",
       "            in_features=256, out_features=128, bias=True\n",
       "            (merged): MergeBuffer()\n",
       "            (frozen_merged): MergeBuffer()\n",
       "            (quanta_weights): ParameterDict(\n",
       "                (-1 -2): Parameter containing: [torch.FloatTensor of size 8x8x8x8]\n",
       "                (-1 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "                (-2 -3): Parameter containing: [torch.FloatTensor of size 12x8x12x8]\n",
       "            )\n",
       "            (quanta_weights2): BufferDict()\n",
       "          )\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(3, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-2): 3 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from quanta import QuanTAConfig, QuanTAModel\n",
    "\n",
    "qconfig = QuanTAConfig(\n",
    "    d=3,\n",
    "    per_dim_features=[12, 8, 8],\n",
    "    target_modules=target_modules,\n",
    "    merge_weights=False,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "quanta_model = QuanTAModel(qconfig, model)\n",
    "quanta_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "154b0c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 94,180,879\n",
      "Trainable parameters: 585,728\n",
      "Frozen parameters: 93,595,151\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "# Example\n",
    "total, trainable = count_parameters(model)\n",
    "print(f\"Total parameters: {total:,}\")\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Frozen parameters: {total - trainable:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
