{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12769691,"sourceType":"datasetVersion","datasetId":8072606}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Git clone finetune-SAM\n\n!git clone https://github.com/mazurowski-lab/finetune-SAM.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade pip\n!pip install --index-url https://download.pytorch.org/whl/cu118 \\\n  torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T03:27:27.268383Z","iopub.execute_input":"2025-08-20T03:27:27.269079Z","iopub.status.idle":"2025-08-20T03:30:01.679961Z","shell.execute_reply.started":"2025-08-20T03:27:27.269052Z","shell.execute_reply":"2025-08-20T03:30:01.679166Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.2-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.2\nLooking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch==2.3.0\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.3.0%2Bcu118-cp311-cp311-linux_x86_64.whl (839.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m839.7/839.7 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.18.0\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.18.0%2Bcu118-cp311-cp311-linux_x86_64.whl (6.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hCollecting torchaudio==2.3.0\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.3.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (4.14.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.3.0) (2025.5.1)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m146.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m128.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==8.7.0.84 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m109.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.20.5 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.20.5-py3-none-manylinux2014_x86_64.whl (142.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.9/142.9 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\nCollecting triton==2.3.0 (from torch==2.3.0)\n  Downloading https://download.pytorch.org/whl/triton-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.0) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.18.0) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.3.0) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.18.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.18.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.18.0) (2022.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.18.0) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.18.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.18.0) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.3.0) (1.3.0)\nInstalling collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchaudio, torchvision\n\u001b[2K  Attempting uninstall: triton\n\u001b[2K    Found existing installation: triton 3.2.0\n\u001b[2K    Uninstalling triton-3.2.0:\n\u001b[2K      Successfully uninstalled triton-3.2.0━━━━━\u001b[0m \u001b[32m 0/15\u001b[0m [triton]\n\u001b[2K  Attempting uninstall: torch━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m11/15\u001b[0m [nvidia-cudnn-cu11]11]1]\n\u001b[2K    Found existing installation: torch 2.6.0+cu124\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]nn-cu11]\n\u001b[2K    Uninstalling torch-2.6.0+cu124:━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n\u001b[2K      Successfully uninstalled torch-2.6.0+cu124m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n\u001b[2K  Attempting uninstall: torchaudio━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n\u001b[2K    Found existing installation: torchaudio 2.6.0+cu124━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n\u001b[2K    Uninstalling torchaudio-2.6.0+cu124:\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m12/15\u001b[0m [torch]\n\u001b[2K      Successfully uninstalled torchaudio-2.6.0+cu124[0m\u001b[90m━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [torchaudio]\n\u001b[2K  Attempting uninstall: torchvision━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [torchaudio]\n\u001b[2K    Found existing installation: torchvision 0.21.0+cu124━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [torchaudio]\n\u001b[2K    Uninstalling torchvision-0.21.0+cu124:\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m13/15\u001b[0m [torchaudio]\n\u001b[2K      Successfully uninstalled torchvision-0.21.0+cu124\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m14/15\u001b[0m [torchvision]\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [torchvision]\u001b[0m [torchvision]\n\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.20.5 nvidia-nvtx-cu11-11.8.86 torch-2.3.0+cu118 torchaudio-2.3.0+cu118 torchvision-0.18.0+cu118 triton-2.3.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%writefile /kaggle/working/requirements.txt\n\nasttokens\ncertifi\ncharset-normalizer\nclick\ncolorama\ncontourpy\ncycler\nDeprecated\neinops\nexecuting\nfilelock\nfonttools\nfsspec\nhuggingface-hub\nhumanize\nicecream\nidna\nimageio\nJinja2\nkiwisolver\nmarkdown-it-py\nMarkupSafe\nmatplotlib\nmdurl\nmonai==1.3.1\nmpmath\nnetworkx\nnibabel\nnptyping\nnumpy==1.26.4\nopencv-python-headless\npackaging\npandas\npillow\nPygments\npynrrd\npyparsing\npython-dateutil\npytz\nPyWavelets\nPyYAML\nrequests\nrich\nsafetensors\nscikit-image\nscipy\nseaborn\nsegment-anything==1.0\nshellingham\nSimpleITK\nsix\nslicerio\nsympy\ntifffile\ntimm==1.0.3\ntorchio==0.19.6\ntqdm\ntyper\ntyping_extensions\ntzdata\nurllib3\nwrapt\ntensorboardX","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -r /kaggle/working/requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T03:30:01.681683Z","iopub.execute_input":"2025-08-20T03:30:01.681980Z","iopub.status.idle":"2025-08-20T03:30:07.953951Z","shell.execute_reply.started":"2025-08-20T03:30:01.681955Z","shell.execute_reply":"2025-08-20T03:30:07.952999Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: asttokens in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 2)) (3.0.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 3)) (2025.6.15)\nRequirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 4)) (3.4.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 5)) (8.2.1)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 6)) (0.4.6)\nRequirement already satisfied: contourpy in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 7)) (1.3.2)\nRequirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 8)) (0.12.1)\nRequirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 9)) (1.2.18)\nRequirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 10)) (0.8.1)\nCollecting executing (from -r /kaggle/working/requirements.txt (line 11))\n  Downloading executing-2.2.0-py2.py3-none-any.whl.metadata (8.9 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 12)) (3.18.0)\nRequirement already satisfied: fonttools in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 13)) (4.58.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 14)) (2025.5.1)\nRequirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 15)) (0.33.1)\nRequirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 16)) (4.12.3)\nCollecting icecream (from -r /kaggle/working/requirements.txt (line 17))\n  Downloading icecream-2.1.7-py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 18)) (3.10)\nRequirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 19)) (2.37.0)\nRequirement already satisfied: Jinja2 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 20)) (3.1.6)\nRequirement already satisfied: kiwisolver in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 21)) (1.4.8)\nRequirement already satisfied: markdown-it-py in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 22)) (3.0.0)\nRequirement already satisfied: MarkupSafe in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 23)) (3.0.2)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 24)) (3.7.2)\nRequirement already satisfied: mdurl in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 25)) (0.1.2)\nCollecting monai==1.3.1 (from -r /kaggle/working/requirements.txt (line 26))\n  Downloading monai-1.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: mpmath in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 27)) (1.3.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 28)) (3.5)\nRequirement already satisfied: nibabel in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 29)) (5.3.2)\nCollecting nptyping (from -r /kaggle/working/requirements.txt (line 30))\n  Downloading nptyping-2.5.0-py3-none-any.whl.metadata (7.6 kB)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 31)) (1.26.4)\nRequirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 32)) (4.11.0.86)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 33)) (25.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 34)) (2.2.3)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 35)) (11.2.1)\nRequirement already satisfied: Pygments in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 36)) (2.19.2)\nCollecting pynrrd (from -r /kaggle/working/requirements.txt (line 37))\n  Downloading pynrrd-1.1.3-py3-none-any.whl.metadata (5.4 kB)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 38)) (3.0.9)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 39)) (2.9.0.post0)\nRequirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 40)) (2025.2)\nRequirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 41)) (1.8.0)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 42)) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 43)) (2.32.4)\nRequirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 44)) (14.0.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 45)) (0.5.3)\nRequirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 46)) (0.25.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 47)) (1.15.3)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 48)) (0.12.2)\nRequirement already satisfied: segment-anything==1.0 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 49)) (1.0)\nRequirement already satisfied: shellingham in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 50)) (1.5.4)\nRequirement already satisfied: SimpleITK in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 51)) (2.5.2)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 52)) (1.17.0)\nCollecting slicerio (from -r /kaggle/working/requirements.txt (line 53))\n  Downloading slicerio-1.1.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 54)) (1.13.1)\nRequirement already satisfied: tifffile in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 55)) (2025.6.11)\nCollecting timm==1.0.3 (from -r /kaggle/working/requirements.txt (line 56))\n  Downloading timm-1.0.3-py3-none-any.whl.metadata (43 kB)\nCollecting torchio==0.19.6 (from -r /kaggle/working/requirements.txt (line 57))\n  Downloading torchio-0.19.6-py2.py3-none-any.whl.metadata (48 kB)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 58)) (4.67.1)\nRequirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 59)) (0.16.0)\nRequirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 60)) (4.14.0)\nRequirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 61)) (2025.2)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 62)) (2.5.0)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from -r /kaggle/working/requirements.txt (line 63)) (1.17.2)\nCollecting tensorboardX (from -r /kaggle/working/requirements.txt (line 64))\n  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: torch>=1.9 in /usr/local/lib/python3.11/dist-packages (from monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (2.3.0+cu118)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (2.4.1)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm==1.0.3->-r /kaggle/working/requirements.txt (line 56)) (0.18.0+cu118)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->-r /kaggle/working/requirements.txt (line 15)) (1.1.5)\nRequirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel->-r /kaggle/working/requirements.txt (line 29)) (6.5.2)\nRequirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image->-r /kaggle/working/requirements.txt (line 46)) (0.4)\nRequirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX->-r /kaggle/working/requirements.txt (line 64)) (3.20.3)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (8.7.0.84)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (11.8.86)\nRequirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.9->monai==1.3.1->-r /kaggle/working/requirements.txt (line 26)) (2.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (2022.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (2024.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy==1.26.4->-r /kaggle/working/requirements.txt (line 31)) (2024.2.0)\n\u001b[33mWARNING: typer 0.16.0 does not provide the extra 'all'\u001b[0m\u001b[33m\n\u001b[0mDownloading monai-1.3.1-py3-none-any.whl (1.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n\u001b[?25hDownloading torchio-0.19.6-py2.py3-none-any.whl (173 kB)\nDownloading executing-2.2.0-py2.py3-none-any.whl (26 kB)\nDownloading icecream-2.1.7-py3-none-any.whl (14 kB)\nDownloading nptyping-2.5.0-py3-none-any.whl (37 kB)\nDownloading pynrrd-1.1.3-py3-none-any.whl (23 kB)\nDownloading slicerio-1.1.2-py3-none-any.whl (21 kB)\nDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\nInstalling collected packages: executing, icecream, pynrrd, torchio, timm, tensorboardX, slicerio, nptyping, monai\n\u001b[2K  Attempting uninstall: timm0m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9\u001b[0m [torchio]\n\u001b[2K    Found existing installation: timm 1.0.15━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9\u001b[0m [torchio]\n\u001b[2K    Uninstalling timm-1.0.15:0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/9\u001b[0m [torchio]\n\u001b[2K      Successfully uninstalled timm-1.0.150m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/9\u001b[0m [timm]\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9/9\u001b[0m [monai]32m8/9\u001b[0m [monai]boardX]\n\u001b[1A\u001b[2KSuccessfully installed executing-2.2.0 icecream-2.1.7 monai-1.3.1 nptyping-2.5.0 pynrrd-1.1.3 slicerio-1.1.2 tensorboardX-2.6.4 timm-1.0.3 torchio-0.19.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Sanity Check\n\nimport torch, torchvision, triton\nprint(torch.__version__, torch.version.cuda, torch.cuda.is_available())\nprint(torch.cuda.get_device_name(0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T04:30:34.577664Z","iopub.execute_input":"2025-08-16T04:30:34.577953Z","iopub.status.idle":"2025-08-16T04:30:42.038407Z","shell.execute_reply.started":"2025-08-16T04:30:34.577928Z","shell.execute_reply":"2025-08-16T04:30:42.037804Z"}},"outputs":[{"name":"stdout","text":"2.3.0+cu118 11.8 True\nTesla T4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Download the ViT-B SAM weights: \n\n!mkdir /kaggle/working/sam_vit_b_weights\n\n!wget -O /kaggle/working/sam_vit_b_weights/sam_vit_b_01ec64.pth \\\n  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make sure the directory exists (RUN ONLY ONCE)\n!mkdir -p /kaggle/working/finetune-SAM/","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/finetune-SAM/models/sam/modeling/image_encoder.py\n\n# Copyright (c) Meta Platforms, Inc. and affiliates.\n# All rights reserved.\n\n# This source code is licensed under the license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom einops import rearrange\nimport math\n\nfrom typing import Optional, Tuple, Type\n\nfrom .common import LayerNorm2d, MLPBlock, Adapter\n\n\n\n#class PromptEncoderViT\n\n# This class and its supporting functions below lightly adapted from the ViTDet backbone available at: https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/vit.py # noqa\nclass ImageEncoderViT(nn.Module):\n    def __init__(\n        self,\n        args,\n        img_size: int = 1024,\n        patch_size: int = 16,\n        in_chans: int = 3,\n        embed_dim: int = 768,\n        depth: int = 12,\n        num_heads: int = 12,\n        mlp_ratio: float = 4.0,\n        out_chans: int = 256,\n        qkv_bias: bool = True,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        use_abs_pos: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        global_attn_indexes: Tuple[int, ...] = (),\n    ) -> None:\n        \"\"\"\n        Args:\n            img_size (int): Input image size.\n            patch_size (int): Patch size.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n            depth (int): Depth of ViT.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            use_abs_pos (bool): If True, use absolute positional embeddings.\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks.\n            global_attn_indexes (list): Indexes for blocks using global attention.\n        \"\"\"\n        super().__init__()\n        self.img_size = img_size\n        self.in_chans = in_chans\n        self.args = args\n        self.depth = depth\n        # self.dev = args.devices\n\n        self.patch_embed = PatchEmbed(\n            kernel_size=(patch_size, patch_size),\n            stride=(patch_size, patch_size),\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n        )\n        # if self.args.if_split_encoder_gpus:\n        #     self.patch_embed = self.patch_embed.to(self.dev[0])\n\n        self.pos_embed: Optional[nn.Parameter] = None\n        if use_abs_pos:\n            # Initialize absolute positional embedding with pretrain image size.\n            self.pos_embed = nn.Parameter(\n                # torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim,dtype=torch.float,device=self.dev[0]))\n                torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim,dtype=torch.float))\n\n        self.blocks = nn.ModuleList()\n        for i in range(depth):\n            block = Block(\n                args= self.args,\n                dim=embed_dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                qkv_bias=qkv_bias,\n                norm_layer=norm_layer,\n                act_layer=act_layer,\n                depth = i,\n                use_rel_pos=use_rel_pos,\n                rel_pos_zero_init=rel_pos_zero_init,\n                window_size=window_size if i not in global_attn_indexes else 0,\n                input_size=(img_size // patch_size, img_size // patch_size),\n            )\n            # if self.args.if_split_encoder_gpus:\n            #     if i<int(self.depth*self.args.gpu_fractions[0]):\n            #         block.to(self.dev[0])\n            #     else:\n            #         block.to(self.dev[1])\n            self.blocks.append(block)\n            \n\n        self.neck = nn.Sequential(\n            nn.Conv2d(\n                embed_dim,\n                out_chans,\n                kernel_size=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n            nn.Conv2d(\n                out_chans,\n                out_chans,\n                kernel_size=3,\n                padding=1,\n                bias=False,\n            ),\n            LayerNorm2d(out_chans),\n        )\n        # if self.args.if_split_encoder_gpus:\n        #     self.neck = self.neck.to(self.dev[1])\n\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.patch_embed(x)\n        if self.pos_embed is not None:\n            x = x + self.pos_embed\n\n        for i,blk in enumerate(self.blocks):\n            # if self.args.if_split_encoder_gpus:\n            #     if i<int(self.depth*self.args.gpu_fractions[0]):\n            #         x = x.to(self.dev[0])\n            #     else:\n            #         x = x.to(self.dev[1])\n            x = blk(x)\n\n        x = self.neck(x.permute(0, 3, 1, 2))\n\n        return x\n\n\nclass Block(nn.Module):\n    \"\"\"Transformer blocks with support of window attention and residual propagation blocks\"\"\"\n\n    def __init__(\n        self,\n        args,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        scale: float = 0.5,\n        qkv_bias: bool = True,\n        norm_layer: Type[nn.Module] = nn.LayerNorm,\n        act_layer: Type[nn.Module] = nn.GELU,\n        depth = 1,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        window_size: int = 0,\n        input_size: Optional[Tuple[int, int]] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads in each ViT block.\n            mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n            qkv_bias (bool): If True, add a learnable bias to query, key, value.\n            norm_layer (nn.Module): Normalization layer.\n            act_layer (nn.Module): Activation layer.\n            depth: the depth of this block\n            use_rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            window_size (int): Window size for window attention blocks. If it equals 0, then\n                use global attention.\n            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\n        super().__init__()\n        self.args = args\n        self.norm1 = norm_layer(dim)\n        self.depth = depth\n        self.attn = Attention(\n            dim,\n            num_heads=num_heads,\n            qkv_bias=qkv_bias,\n            use_rel_pos=use_rel_pos,\n            rel_pos_zero_init=rel_pos_zero_init,\n            input_size=input_size if window_size == 0 else (window_size, window_size),\n        )\n        if self.args.if_encoder_adapter and (self.depth in self.args.encoder_adapter_depths):\n            self.MLP_Adapter = Adapter(dim, skip_connect=False)  # MLP-adapter, no skip connection\n            self.Space_Adapter = Adapter(dim)  # with skip connection\n            self.scale = scale\n            self.Depth_Adapter = Adapter(dim, skip_connect=False)  # no skip connection\n\n        self.norm2 = norm_layer(dim)\n        self.mlp = MLPBlock(embedding_dim=dim, mlp_dim=int(dim * mlp_ratio), act=act_layer)\n\n        self.window_size = window_size\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        shortcut = x\n        # Window partition\n        if self.window_size > 0:\n            H, W = x.shape[1], x.shape[2]\n            x, pad_hw = window_partition(x, self.window_size)\n\n        ## 3d branch\n        if self.args.thd: \n            print('add 3D branch')\n            hh, ww = x.shape[1], x.shape[2]\n            depth = self.args.chunk\n            xd = rearrange(x, '(b d) h w c -> (b h w) d c ', d=depth)\n            # xd = rearrange(xd, '(b d) n c -> (b n) d c', d=self.in_chans)\n            xd = self.norm1(xd)\n            dh, _ = closest_numbers(depth)\n            xd = rearrange(xd, 'bhw (dh dw) c -> bhw dh dw c', dh= dh)\n            xd = self.Depth_Adapter(self.attn(xd))\n            xd = rearrange(xd, '(b n) dh dw c ->(b dh dw) n c', n= hh * ww )\n\n        x = self.norm1(x)\n        x = self.attn(x)\n        if self.args.if_encoder_adapter and (self.depth in self.args.encoder_adapter_depths):\n            #print('add adapter layer')\n            x = self.Space_Adapter(x)\n\n        if self.args.thd:\n            xd = rearrange(xd, 'b (hh ww) c -> b  hh ww c', hh= hh )\n            x = x + xd\n        # Reverse window partition\n        if self.window_size > 0:\n            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n\n        x = shortcut + x\n        xn = self.norm2(x)\n        if self.args.if_encoder_adapter and (self.depth in self.args.encoder_adapter_depths):\n            x = x + self.mlp(xn) + self.scale * self.MLP_Adapter(xn)\n        else:\n            x = x + self.mlp(xn)\n        return x\n\n\nclass Attention(nn.Module):\n    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 8,\n        qkv_bias: bool = True,\n        use_rel_pos: bool = False,\n        rel_pos_zero_init: bool = True,\n        input_size: Optional[Tuple[int, int]] = None,\n    ) -> None:\n        \"\"\"\n        Args:\n            dim (int): Number of input channels.\n            num_heads (int): Number of attention heads.\n            qkv_bias (bool):  If True, add a learnable bias to query, key, value.\n            rel_pos (bool): If True, add relative positional embeddings to the attention map.\n            rel_pos_zero_init (bool): If True, zero initialize relative positional parameters.\n            input_size (tuple(int, int) or None): Input resolution for calculating the relative\n                positional parameter size.\n        \"\"\"\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = head_dim**-0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.proj = nn.Linear(dim, dim)\n\n        self.use_rel_pos = use_rel_pos\n        if self.use_rel_pos:\n            assert (\n                input_size is not None\n            ), \"Input size must be provided if using relative positional encoding.\"\n            # initialize relative positional embeddings\n            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        B, H, W, _ = x.shape\n        # qkv with shape (3, B, nHead, H * W, C)\n        qkv = self.qkv(x).reshape(B, H * W, 3, self.num_heads, -1).permute(2, 0, 3, 1, 4)\n        # q, k, v with shape (B * nHead, H * W, C)\n        q, k, v = qkv.reshape(3, B * self.num_heads, H * W, -1).unbind(0)\n\n        attn = (q * self.scale) @ k.transpose(-2, -1)\n\n        if self.use_rel_pos:\n            attn = add_decomposed_rel_pos(attn, q, self.rel_pos_h, self.rel_pos_w, (H, W), (H, W))\n\n        attn = attn.softmax(dim=-1)\n        x = (attn @ v).view(B, self.num_heads, H, W, -1).permute(0, 2, 3, 1, 4).reshape(B, H, W, -1)\n        x = self.proj(x)\n\n        return x\n\n\ndef window_partition(x: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n    \"\"\"\n    Partition into non-overlapping windows with padding if needed.\n    Args:\n        x (tensor): input tokens with [B, H, W, C].\n        window_size (int): window size.\n\n    Returns:\n        windows: windows after partition with [B * num_windows, window_size, window_size, C].\n        (Hp, Wp): padded height and width before partition\n    \"\"\"\n    B, H, W, C = x.shape\n\n    pad_h = (window_size - H % window_size) % window_size\n    pad_w = (window_size - W % window_size) % window_size\n    if pad_h > 0 or pad_w > 0:\n        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n    Hp, Wp = H + pad_h, W + pad_w\n\n    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows, (Hp, Wp)\n\n\ndef window_unpartition(\n    windows: torch.Tensor, window_size: int, pad_hw: Tuple[int, int], hw: Tuple[int, int]\n) -> torch.Tensor:\n    \"\"\"\n    Window unpartition into original sequences and removing padding.\n    Args:\n        windows (tensor): input tokens with [B * num_windows, window_size, window_size, C].\n        window_size (int): window size.\n        pad_hw (Tuple): padded height and width (Hp, Wp).\n        hw (Tuple): original height and width (H, W) before padding.\n\n    Returns:\n        x: unpartitioned sequences with [B, H, W, C].\n    \"\"\"\n    Hp, Wp = pad_hw\n    H, W = hw\n    B = windows.shape[0] // (Hp * Wp // window_size // window_size)\n    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)\n\n    if Hp > H or Wp > W:\n        x = x[:, :H, :W, :].contiguous()\n    return x\n\n\ndef get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Get relative positional embeddings according to the relative positions of\n        query and key sizes.\n    Args:\n        q_size (int): size of query q.\n        k_size (int): size of key k.\n        rel_pos (Tensor): relative position embeddings (L, C).\n\n    Returns:\n        Extracted positional embeddings according to relative positions.\n    \"\"\"\n    max_rel_dist = int(2 * max(q_size, k_size) - 1)\n    # Interpolate rel pos if needed.\n    if rel_pos.shape[0] != max_rel_dist:\n        # Interpolate rel pos.\n        rel_pos_resized = F.interpolate(\n            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n            size=max_rel_dist,\n            mode=\"linear\",\n        )\n        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n    else:\n        rel_pos_resized = rel_pos\n\n    # Scale the coords with short length if shapes for q and k are different.\n    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n\n    return rel_pos_resized[relative_coords.long()]\n\n\ndef add_decomposed_rel_pos(\n    attn: torch.Tensor,\n    q: torch.Tensor,\n    rel_pos_h: torch.Tensor,\n    rel_pos_w: torch.Tensor,\n    q_size: Tuple[int, int],\n    k_size: Tuple[int, int],\n) -> torch.Tensor:\n    \"\"\"\n    Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n    https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py   # noqa B950\n    Args:\n        attn (Tensor): attention map.\n        q (Tensor): query q in the attention layer with shape (B, q_h * q_w, C).\n        rel_pos_h (Tensor): relative position embeddings (Lh, C) for height axis.\n        rel_pos_w (Tensor): relative position embeddings (Lw, C) for width axis.\n        q_size (Tuple): spatial sequence size of query q with (q_h, q_w).\n        k_size (Tuple): spatial sequence size of key k with (k_h, k_w).\n\n    Returns:\n        attn (Tensor): attention map with added relative positional embeddings.\n    \"\"\"\n    q_h, q_w = q_size\n    k_h, k_w = k_size\n    Rh = get_rel_pos(q_h, k_h, rel_pos_h)\n    Rw = get_rel_pos(q_w, k_w, rel_pos_w)\n\n    B, _, dim = q.shape\n    r_q = q.reshape(B, q_h, q_w, dim)\n    rel_h = torch.einsum(\"bhwc,hkc->bhwk\", r_q, Rh)\n    rel_w = torch.einsum(\"bhwc,wkc->bhwk\", r_q, Rw)\n\n    attn = (\n        attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n    ).view(B, q_h * q_w, k_h * k_w)\n\n    return attn\n\ndef closest_numbers(target):\n    a = int(target ** 0.5)\n    b = a + 1\n    while True:\n        if a * b == target:\n            return (a, b)\n        elif a * b < target:\n            b += 1\n        else:\n            a -= 1\n\n\nclass PatchEmbed(nn.Module):\n    \"\"\"\n    Image to Patch Embedding.\n    \"\"\"\n\n    def __init__(\n        self,\n        kernel_size: Tuple[int, int] = (16, 16),\n        stride: Tuple[int, int] = (16, 16),\n        padding: Tuple[int, int] = (0, 0),\n        in_chans: int = 3,\n        embed_dim: int = 768,\n    ) -> None:\n        \"\"\"\n        Args:\n            kernel_size (Tuple): kernel size of the projection layer.\n            stride (Tuple): stride of the projection layer.\n            padding (Tuple): padding size of the projection layer.\n            in_chans (int): Number of input image channels.\n            embed_dim (int): Patch embedding dimension.\n        \"\"\"\n        super().__init__()\n\n        self.proj = nn.Conv2d(\n            in_chans, embed_dim, kernel_size=kernel_size, stride=stride, padding=padding\n        )\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.proj(x)\n        # B C H W -> B H W C\n        x = x.permute(0, 2, 3, 1)\n        return x\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/finetune-SAM/cfg.py\n\nimport argparse\n\ndef parse_args():    \n    parser = argparse.ArgumentParser()\n    parser.add_argument('-net', type=str, default='sam', help='net type')\n    parser.add_argument('-arch', type=str, default='vit_b', help='net architecture, pick between vit_h, vit_b, vit_t')\n    parser.add_argument('-baseline', type=str, default='unet', help='baseline net type')\n    parser.add_argument('-dataset_name', type=str, default='MRI-Prostate', help='the name of dataset to be finetuned')\n    \n    parser.add_argument('-img_folder', type=str, default='./datasets/', help='the folder putting images')\n    parser.add_argument('-mask_folder', type=str, default='./datasets/', help='the folder putting masks')\n    parser.add_argument('-train_img_list', type=str, default='./datasets/train.csv')\n    parser.add_argument('-val_img_list', type=str,default='./datasets/val.csv')\n    parser.add_argument('-test_img_list', type=str, default='./datasets/train.csv')\n    parser.add_argument('-targets', type=str,default='combine_all')\n\n    parser.add_argument('-finetune_type', type=str, default='adapter', help='normalization type, pick among vanilla,adapter,lora')\n    parser.add_argument('-normalize_type', type=str, default='sam', help='normalization type, pick between sam or medsam')\n    \n    parser.add_argument('-dir_checkpoint', type=str, default='checkpoints', help='the checkpoint folder to save final model')\n    parser.add_argument('-num_cls', type=int, default=2, help='the number of output channels (need to be your target cls num +1)')\n    parser.add_argument('-epochs', type=int, default=200, help='the number of largest epochs to train')\n    parser.add_argument('-sam_ckpt', type=str, default='sam_vit_b_01ec64.pth', help='the path to the checkpoint to load')\n    \n    parser.add_argument('-type', type=str, default='map', help='condition type:ave,rand,rand_map')\n    parser.add_argument('-vis', type=int, default=None, help='visualization')\n    parser.add_argument('-reverse', type=bool, default=False, help='adversary reverse')\n    parser.add_argument('-pretrain', type=bool, default=False, help='adversary reverse')\n    parser.add_argument('-val_freq',type=int,default=100,help='interval between each validation')\n    parser.add_argument('-gpu', type=bool, default=True, help='use gpu or not')\n    parser.add_argument('-gpu_device', type=int, default=0, help='use which gpu')\n    parser.add_argument('-sim_gpu', type=int, default=0, help='split sim to this gpu')\n    parser.add_argument('-epoch_ini', type=int, default=1, help='start epoch')\n    parser.add_argument('-image_size', type=int, default=1024, help='image_size')\n    parser.add_argument('-out_size', type=int, default=256, help='output_size')\n    parser.add_argument('-patch_size', type=int, default=2, help='patch_size')\n    parser.add_argument('-dim', type=int, default=512, help='dim_size')\n    parser.add_argument('-depth', type=int, default=64, help='depth')\n    parser.add_argument('-heads', type=int, default=16, help='heads number')\n    parser.add_argument('-mlp_dim', type=int, default=1024, help='mlp_dim')\n    parser.add_argument('-w', type=int, default=4, help='number of workers for dataloader')\n    parser.add_argument('-b', type=int, default=4, help='batch size for dataloader')\n    parser.add_argument('-s', type=bool, default=True, help='whether shuffle the dataset')\n    parser.add_argument('-if_warmup', type=bool, default=False, help='if warm up training phase')\n    parser.add_argument('-warmup_period', type=int, default=200, help='warm up training phase')\n    parser.add_argument('-lr', type=float, default=1e-3, help='initial learning rate')\n    parser.add_argument('-uinch', type=int, default=1, help='input channel of unet')\n    parser.add_argument('-imp_lr', type=float, default=3e-4, help='implicit learning rate')\n    parser.add_argument('-weights', type=str, default = 0, help='the weights file you want to test')\n    parser.add_argument('-base_weights', type=str, default = 0, help='the weights baseline')\n    parser.add_argument('-sim_weights', type=str, default = 0, help='the weights sim')\n    parser.add_argument('-distributed', default='none' ,type=str,help='multi GPU ids to use')\n    parser.add_argument('-dataset', default='isic' ,type=str,help='dataset name')\n    parser.add_argument('-thd', type=bool, default=False , help='3d or not')\n    parser.add_argument('-chunk', type=int, default=96 , help='crop volume depth')\n    parser.add_argument('-num_sample', type=int, default=4 , help='sample pos and neg')\n    parser.add_argument('-roi_size', type=int, default=96 , help='resolution of roi')\n\n    parser.add_argument('-if_update_encoder', type=bool, default=False , help='if update_image_encoder')\n    parser.add_argument('-if_encoder_adapter', type=bool, default=False , help='if add adapter to encoder')\n    \n    parser.add_argument('-encoder-adapter-depths', type=list, default=[0,1,10,11] , help='the depth of blocks to add adapter')\n    parser.add_argument('-if_mask_decoder_adapter', type=bool, default=False , help='if add adapter to mask decoder')\n    parser.add_argument('-decoder_adapt_depth', type=int, default=2, help='the depth of the decoder adapter')\n    \n    parser.add_argument('-if_encoder_lora_layer', type=bool, default=False , help='if add lora to encoder')\n    parser.add_argument('-if_decoder_lora_layer', type=bool, default=False , help='if add lora to decoder')\n    parser.add_argument('-encoder_lora_layer', type=list, default=[0,1,10,11] , help='the depth of blocks to add lora, if [], it will add at each layer')\n    \n    parser.add_argument('-if_split_encoder_gpus', type=bool, default=False , help='if split encoder to multiple gpus')\n    parser.add_argument('-devices', type=list, default=[0,1] , help='if split encoder to multiple gpus')\n    parser.add_argument('-gpu_fractions', type=list, default=[0.5,0.5] , help='how to split encoder to multiple gpus')\n    \n  \n    parser.add_argument('-evl_chunk', type=int, default=None , help='evaluation chunk')\n    opt = parser.parse_args()\n\n    return opt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T03:31:00.951215Z","iopub.execute_input":"2025-08-20T03:31:00.951953Z","iopub.status.idle":"2025-08-20T03:31:00.959993Z","shell.execute_reply.started":"2025-08-20T03:31:00.951920Z","shell.execute_reply":"2025-08-20T03:31:00.959144Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/finetune-SAM/cfg.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile /kaggle/working/finetune-SAM/DDP_splitgpu_train_finetune_noprompt.py\n\n#from segment_anything import SamPredictor, sam_model_registry\nfrom models.sam import SamPredictor, sam_model_registry\nfrom models.sam.utils.transforms import ResizeLongestSide\nfrom skimage.measure import label\nfrom models.sam_LoRa import LoRA_Sam\n#Scientific computing \nimport numpy as np\nimport os\n#Pytorch packages\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets\nfrom tensorboardX import SummaryWriter\n#Visulization\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom PIL import Image\n#Others\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport copy\nfrom utils.dataset import Public_dataset\nimport torch.nn.functional as F\nfrom torch.nn.functional import one_hot\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom utils.losses import DiceLoss\nfrom utils.dsc import dice_coeff\nimport cv2\nimport monai\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom utils.utils import vis_image\nimport cfg\nargs = cfg.parse_args()\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\"\"\"\ndef setup(rank, world_size, model_basic, trainloader, valloader,dir_checkpoint, backend='nccl'): \n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12333'\n    # initialize the process group\n    dist.init_process_group(backend, rank=rank, world_size=world_size)\n\n    # Give the DataLoaders the samplers so they serve unique data slices\n    trainloader.sampler.set_epoch(0) # You can set this to the current epoch in the training loop\n    valloader.sampler.set_epoch(0)\n    model_basic(args,rank, world_size,trainloader,valloader,dir_checkpoint)\n\"\"\"\ndef setup(rank, world_size, model_basic_fn, train_dataset, eval_dataset, dir_checkpoint, backend='nccl'):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12333'\n    dist.init_process_group(backend, rank=rank, world_size=world_size)\n\n    # Pass the datasets down to the training function\n    model_basic_fn(args, rank, world_size, train_dataset, eval_dataset, dir_checkpoint)\n\n                    \ndef model_basic(args,rank, world_size,trainloader,valloader,dir_checkpoint):\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    \n    args.devices = [dev0,dev1]\n    \n    if args.if_warmup:\n        b_lr = args.lr / args.warmup_period\n    else:\n        b_lr = args.lr\n    \n\n    epochs = args.epochs\n    iter_num = 0\n    max_iterations = epochs * len(trainloader) \n    writer = SummaryWriter(dir_checkpoint + '/log')\n    \n    print(f\"Running basic DDP example on rank {rank}.\")\n    # create model and move it to GPU with id rank\n    model = sam_model_registry[\"vit_b\"](args,checkpoint=args.sam_ckpt,num_classes=2)\n    #print(model)\n\n    if args.finetune_type == 'adapter':\n        for n, value in model.named_parameters():\n            if \"Adapter\" not in n: # only update parameters in adapter\n                value.requires_grad = False\n    elif args.finetune_type == 'vanilla' and args.if_update_encoder==False:      \n        for n, value in model.image_encoder.named_parameters():\n            value.requires_grad = False\n    elif args.finetune_type == 'lora':\n        model = LoRA_Sam(args,model,r=4).sam\n        \n    \n    ddp_model = DDP(model)\n    \n    optimizer = optim.AdamW(ddp_model.parameters(), lr=b_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1, amsgrad=False)\n    optimizer.zero_grad()\n    criterion1 = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, to_onehot_y=True,reduction='mean')\n    criterion2 = nn.CrossEntropyLoss()\n    pbar = tqdm(range(epochs))\n    val_largest_dsc = 0\n    last_update_epoch = 0\n    for epoch in pbar:\n        ddp_model.train()\n        train_loss = 0\n        for i,data in enumerate(trainloader):\n            imgs = data['image'].to(dev0)\n            msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n            msks = msks.to(dev1) # output will be in device 1\n            img_emb= ddp_model.module.image_encoder(imgs)\n            sparse_emb, dense_emb = ddp_model.module.prompt_encoder(\n            points=None,\n            boxes=None,\n            masks=None,\n            )\n            pred, _ = ddp_model.module.mask_decoder(\n                            image_embeddings=img_emb,\n                            image_pe=ddp_model.module.prompt_encoder.get_dense_pe(), \n                            sparse_prompt_embeddings=sparse_emb,\n                            dense_prompt_embeddings=dense_emb, \n                            multimask_output=True,\n                          )\n            \n            loss_dice = criterion1(pred,msks.float()) \n            loss_ce = criterion2(pred,torch.squeeze(msks.long(),1))\n            loss =  loss_dice + loss_ce\n            \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            \n            if args.if_warmup and iter_num < args.warmup_period:\n                lr_ = args.lr * ((iter_num + 1) / args.warmup_period)\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = lr_\n\n            else:\n                if args.if_warmup:\n                    shift_iter = iter_num - args.warmup_period\n                    assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n                    lr_ = args.lr * (1.0 - shift_iter / max_iterations) ** 0.9  # learning rate adjustment depends on the max iterations\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = lr_\n\n            train_loss += loss.item()\n            iter_num+=1\n            writer.add_scalar('info/lr', lr_, iter_num)\n            writer.add_scalar('info/total_loss', loss, iter_num)\n            writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n            writer.add_scalar('info/loss_dice', loss_dice, iter_num)\n\n        train_loss /= (i+1)\n        pbar.set_description('Epoch num {}| train loss {} \\n'.format(epoch,train_loss))\n\n        if epoch%2==0:\n            eval_loss=0\n            dsc = 0\n            ddp_model.eval()\n            with torch.no_grad():\n                for i,data in enumerate(valloader):\n                    imgs = data['image'].to(dev0)\n                    msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n                    msks = msks.to(dev1)\n                    img_emb= ddp_model.module.image_encoder(imgs)\n                    sparse_emb, dense_emb = ddp_model.module.prompt_encoder(\n                    points=None,\n                    boxes=None,\n                    masks=None,\n                    )\n                    pred, _ = ddp_model.module.mask_decoder(\n                                    image_embeddings=img_emb,\n                                    image_pe=ddp_model.module.prompt_encoder.get_dense_pe(), \n                                    sparse_prompt_embeddings=sparse_emb,\n                                    dense_prompt_embeddings=dense_emb, \n                                    multimask_output=True,\n                                  )\n            \n                    loss = criterion1(pred,msks.float()) + criterion2(pred,torch.squeeze(msks.long(),1))\n                    eval_loss +=loss.item()\n                    dsc_batch = dice_coeff((pred[:,1,:,:].cpu()>0).long(),msks.cpu().long()).item()\n                    dsc+=dsc_batch\n\n                eval_loss /= (i+1)\n                dsc /= (i+1)\n                writer.add_scalar('eval/loss', eval_loss, epoch)\n                writer.add_scalar('eval/dice', dsc, epoch)\n                \n                print('***Eval Epoch num {} | val loss {} | dsc {} \\n'.format(epoch,eval_loss,dsc))\n                if dsc>val_largest_dsc:\n                    val_largest_dsc = dsc\n                    last_update_epoch = epoch\n                    print('largest DSC now: {}'.format(dsc))\n                    Path(dir_checkpoint).mkdir(parents=True,exist_ok = True)\n                    torch.save(ddp_model.module.state_dict(),dir_checkpoint + '/checkpoint_best.pth')\n                elif (epoch-last_update_epoch)>20:\n                    print('Training finished####################')\n                    # the network haven't been updated for 20 epochs\n                    break\n                    \n    writer.close()   \n    cleanup()\n    \n\ndef model_basic_lora(args,rank, world_size,train_dataset,val_dataset,dir_checkpoint):\n    device = rank\n    \n    # --- ADD THIS BLOCK HERE ---\n    # 1. Now that we are in a distributed process, create the samplers\n    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n    \n    # 2. Create the dataloaders using the samplers\n    trainloader = DataLoader(train_dataset, batch_size=args.b, shuffle=False, num_workers=0, sampler=train_sampler, pin_memory=True)\n    valloader = DataLoader(val_dataset, batch_size=args.b, shuffle=False, num_workers=0, sampler=val_sampler, pin_memory=True)\n    # --- END OF ADDED BLOCK ---\n\n    \n    if args.if_warmup:\n        b_lr = args.lr / args.warmup_period\n    else:\n        b_lr = args.lr\n    \n\n    epochs = args.epochs\n    iter_num = 0\n    max_iterations = epochs * len(trainloader) \n    if rank==0:\n        writer = SummaryWriter(dir_checkpoint + '/log')\n    \n    print(f\"Running basic DDP example on rank {rank}.\")\n    # create model and move it to GPU with id rank\n    model = sam_model_registry[\"vit_b\"](args,checkpoint=args.sam_ckpt,num_classes=2)\n    #print(model)\n\n    model = LoRA_Sam(args,model,r=4).sam\n    \n    model.to(device)\n    \n    # --- MODIFY THIS LINE ---\n    # Wrap the model with DDP and tell it which device to use\n    ddp_model = DDP(model, device_ids=[device], output_device=device)\n    \n    optimizer = optim.AdamW(ddp_model.parameters(), lr=b_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1, amsgrad=False)\n    optimizer.zero_grad()\n    criterion1 = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, to_onehot_y=True,reduction='mean')\n    criterion2 = nn.CrossEntropyLoss()\n    pbar = tqdm(range(epochs))\n    val_largest_dsc = 0\n    last_update_epoch = 0\n    for epoch in pbar:\n        trainloader.sampler.set_epoch(epoch)\n        ddp_model.train()\n        train_loss = 0\n        for i,data in enumerate(trainloader):\n            imgs = data['image'].to(device)\n            msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n            msks = msks.to(device) # output will be in device 1\n            img_emb= ddp_model.module.image_encoder(imgs)\n            sparse_emb, dense_emb = ddp_model.module.prompt_encoder(\n                points=None,\n                boxes=None,\n                masks=None,\n            )\n            pred, _ = ddp_model.module.mask_decoder(\n                            image_embeddings=img_emb,\n                            image_pe=ddp_model.module.prompt_encoder.get_dense_pe(), \n                            sparse_prompt_embeddings=sparse_emb,\n                            dense_prompt_embeddings=dense_emb, \n                            multimask_output=True,\n                          )\n            \n            loss_dice = criterion1(pred,msks.float()) \n            loss_ce = criterion2(pred,torch.squeeze(msks.long(),1))\n            loss =  loss_dice + loss_ce\n            \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            \n            if args.if_warmup and iter_num < args.warmup_period:\n                lr_ = args.lr * ((iter_num + 1) / args.warmup_period)\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = lr_\n\n            else:\n                if args.if_warmup:\n                    shift_iter = iter_num - args.warmup_period\n                    assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n                    lr_ = args.lr * (1.0 - shift_iter / max_iterations) ** 0.9  # learning rate adjustment depends on the max iterations\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = lr_\n\n            train_loss += loss.item()\n            iter_num+=1\n            if rank==0:\n                writer.add_scalar('info/lr', lr_, iter_num)\n                writer.add_scalar('info/total_loss', loss, iter_num)\n                writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n                writer.add_scalar('info/loss_dice', loss_dice, iter_num)\n\n        train_loss /= (i+1)\n        if rank==0:\n            pbar.set_description('Epoch num {}| train loss {} \\n'.format(epoch,train_loss))\n\n        if epoch%2==0:\n            eval_loss=0\n            dsc = 0\n            ddp_model.eval()\n            with torch.no_grad():\n                for i,data in enumerate(valloader):\n                    imgs = data['image'].to(device)\n                    msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n                    msks = msks.to(device)\n                    img_emb= ddp_model.module.image_encoder(imgs)\n                    sparse_emb, dense_emb = ddp_model.module.prompt_encoder(\n                        points=None,\n                        boxes=None,\n                        masks=None,\n                    )\n                    pred, _ = ddp_model.module.mask_decoder(\n                                    image_embeddings=img_emb,\n                                    image_pe=ddp_model.module.prompt_encoder.get_dense_pe(), \n                                    sparse_prompt_embeddings=sparse_emb,\n                                    dense_prompt_embeddings=dense_emb, \n                                    multimask_output=True,\n                                  )\n            \n                    loss = criterion1(pred,msks.float()) + criterion2(pred,torch.squeeze(msks.long(),1))\n                    eval_loss +=loss.item()\n                    dsc_batch = dice_coeff((pred[:,1,:,:].cpu()>0).long(),msks.cpu().long()).item()\n                    dsc+=dsc_batch\n\n                eval_loss /= (i+1)\n                dsc /= (i+1)\n                if rank==0:\n                    writer.add_scalar('eval/loss', eval_loss, epoch)\n                    writer.add_scalar('eval/dice', dsc, epoch)\n                    \n                    print('***Eval Epoch num {} | val loss {} | dsc {} \\n'.format(epoch,eval_loss,dsc))\n                    if dsc>val_largest_dsc:\n                        val_largest_dsc = dsc\n                        last_update_epoch = epoch\n                        print('largest DSC now: {}'.format(dsc))\n                        Path(dir_checkpoint).mkdir(parents=True,exist_ok = True)\n                        torch.save(ddp_model.module.state_dict(),dir_checkpoint + '/checkpoint_best.pth')\n                    elif (epoch-last_update_epoch)>20:\n                        print('Training finished####################')\n                        # the network haven't been updated for 20 epochs\n                        break\n                    \n    if rank==0:\n        writer.close()   \n    cleanup()\n\n\n#def run_demo(demo_fn, size, model_basic,trainloader,valloader,dir_checkpoint):\n#    mp.spawn(demo_fn,\n#             args=(size, model_basic if args.finetune_type!=\"lora\" else model_basic_lora,trainloader,valloader,dir_checkpoint),\n#             nprocs=size,\n#             join=True)\n\ndef run_demo(demo_fn, size, model_basic_fn, train_dataset, eval_dataset, dir_checkpoint):\n    mp.spawn(demo_fn,\n             # Pass the datasets in the args tuple\n             args=(size, model_basic_fn, train_dataset, eval_dataset, dir_checkpoint),\n             nprocs=size,\n             join=True)\n    \nif __name__ == \"__main__\":\n    dataset_name = args.dataset_name\n    print('train dataset: {}'.format(dataset_name)) \n    #train_img_list = args.img_folder + dataset_name + '/train_5shot.csv'\n    #val_img_list = args.img_folder + dataset_name + '/val_5shot.csv'\n    # train_img_list = \"/kaggle/input/xrayhip/train.csv\"\n    # val_img_list = \"/kaggle/input/xrayhip/val.csv\"\n\n    num_workers = 0\n    if_vis = True\n\n    n_gpus = torch.cuda.device_count()\n    # For a 2xT4 machine, this will be 2.\n    size = n_gpus\n    \n    train_dataset = Public_dataset(args,args.img_folder, args.mask_folder, args.train_img_list,phase='train',targets=[f'{args.targets}'],normalize_type='sam',if_prompt=False)\n    val_dataset = Public_dataset(args,args.img_folder, args.mask_folder, args.val_img_list,phase='val',targets=[f'{args.targets}'],normalize_type='sam',if_prompt=False)\n\n    # train_sampler = DistributedSampler(train_dataset, shuffle=True)\n    # val_sampler = DistributedSampler(eval_dataset)\n    \n    # trainloader = DataLoader(train_dataset, batch_size=args.b, shuffle=False, num_workers=num_workers, sampler=train_sampler)\n    # valloader = DataLoader(eval_dataset, batch_size=args.b, shuffle=False, num_workers=num_workers, sampler=val_sampler)\n\n    #processes = []\n    #mp.set_start_method('spawn')\n\n\n    run_demo(setup, size, model_basic_lora if args.,train_dataset,val_dataset,args.dir_checkpoint)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile /kaggle/working/finetune-SAM/DDP_splitgpu_train_finetune_noprompt.py\n\n#from segment_anything import SamPredictor, sam_model_registry\nfrom models.sam import SamPredictor, sam_model_registry\nfrom models.sam.utils.transforms import ResizeLongestSide\nfrom skimage.measure import label\nfrom models.sam_LoRa import LoRA_Sam\n#Scientific computing \nimport numpy as np\nimport os\n#Pytorch packages\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets\nfrom tensorboardX import SummaryWriter\n#Visulization\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom PIL import Image\n#Others\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport copy\nfrom utils.dataset import Public_dataset\nimport torch.nn.functional as F\nfrom torch.nn.functional import one_hot\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom utils.losses import DiceLoss\nfrom utils.dsc import dice_coeff\nimport cv2\nimport monai\nimport torch.multiprocessing as mp\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data.distributed import DistributedSampler\nfrom utils.utils import vis_image\nimport cfg\nargs = cfg.parse_args()\n\ndef cleanup():\n    dist.destroy_process_group()\n\n\ndef setup(rank, world_size, model_basic_fn, train_dataset, eval_dataset, dir_checkpoint, backend='nccl'):\n    os.environ['MASTER_ADDR'] = 'localhost'\n    os.environ['MASTER_PORT'] = '12333'\n    dist.init_process_group(backend, rank=rank, world_size=world_size)\n\n    # Pass the datasets down to the training function\n    model_basic_fn(args, rank, world_size, train_dataset, eval_dataset, dir_checkpoint)\n                    \ndef model_basic(args,rank, world_size,trainloader,valloader,dir_checkpoint):\n    dev0 = rank * 2\n    dev1 = rank * 2 + 1\n    \n    args.devices = [dev0,dev1]\n    \n    if args.if_warmup:\n        b_lr = args.lr / args.warmup_period\n    else:\n        b_lr = args.lr\n    \n\n    epochs = args.epochs\n    iter_num = 0\n    max_iterations = epochs * len(trainloader) \n    writer = SummaryWriter(dir_checkpoint + '/log')\n    \n    print(f\"Running basic DDP example on rank {rank}.\")\n    # create model and move it to GPU with id rank\n    model = sam_model_registry[\"vit_b\"](args,checkpoint=args.sam_ckpt,num_classes=2)\n    #print(model)\n\n    if args.finetune_type == 'adapter':\n        for n, value in model.named_parameters():\n            if \"Adapter\" not in n: # only update parameters in adapter\n                value.requires_grad = False\n    elif args.finetune_type == 'vanilla' and args.if_update_encoder==False:      \n        for n, value in model.image_encoder.named_parameters():\n            value.requires_grad = False\n    elif args.finetune_type == 'lora':\n        model = LoRA_Sam(args,model,r=4).sam\n        \n    \n    ddp_model = DDP(model)\n    \n    optimizer = optim.AdamW(ddp_model.parameters(), lr=b_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1, amsgrad=False)\n    optimizer.zero_grad()\n    criterion1 = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, to_onehot_y=True,reduction='mean')\n    criterion2 = nn.CrossEntropyLoss()\n    pbar = tqdm(range(epochs))\n    val_largest_dsc = 0\n    last_update_epoch = 0\n    for epoch in pbar:\n        ddp_model.train()\n        train_loss = 0\n        for i,data in enumerate(trainloader):\n            imgs = data['image'].to(dev0)\n            msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n            msks = msks.to(dev1) # output will be in device 1\n            img_emb= ddp_model.module.image_encoder(imgs)\n            sparse_emb, dense_emb = ddp_model.module.prompt_encoder(\n            points=None,\n            boxes=None,\n            masks=None,\n            )\n            pred, _ = ddp_model.module.mask_decoder(\n                            image_embeddings=img_emb,\n                            image_pe=ddp_model.module.prompt_encoder.get_dense_pe(), \n                            sparse_prompt_embeddings=sparse_emb,\n                            dense_prompt_embeddings=dense_emb, \n                            multimask_output=True,\n                          )\n            \n            loss_dice = criterion1(pred,msks.float()) \n            loss_ce = criterion2(pred,torch.squeeze(msks.long(),1))\n            loss =  loss_dice + loss_ce\n            \n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad(set_to_none=True)\n            \n            if args.if_warmup and iter_num < args.warmup_period:\n                lr_ = args.lr * ((iter_num + 1) / args.warmup_period)\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = lr_\n\n            else:\n                if args.if_warmup:\n                    shift_iter = iter_num - args.warmup_period\n                    assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n                    lr_ = args.lr * (1.0 - shift_iter / max_iterations) ** 0.9  # learning rate adjustment depends on the max iterations\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = lr_\n\n            train_loss += loss.item()\n            iter_num+=1\n            writer.add_scalar('info/lr', lr_, iter_num)\n            writer.add_scalar('info/total_loss', loss, iter_num)\n            writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n            writer.add_scalar('info/loss_dice', loss_dice, iter_num)\n\n        train_loss /= (i+1)\n        pbar.set_description('Epoch num {}| train loss {} \\n'.format(epoch,train_loss))\n\n        if epoch%2==0:\n            eval_loss=0\n            dsc = 0\n            ddp_model.eval()\n            with torch.no_grad():\n                for i,data in enumerate(valloader):\n                    imgs = data['image'].to(dev0)\n                    msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n                    msks = msks.to(dev1)\n                    img_emb= ddp_model.module.image_encoder(imgs)\n                    sparse_emb, dense_emb = ddp_model.module.prompt_encoder(\n                    points=None,\n                    boxes=None,\n                    masks=None,\n                    )\n                    pred, _ = ddp_model.module.mask_decoder(\n                                    image_embeddings=img_emb,\n                                    image_pe=ddp_model.module.prompt_encoder.get_dense_pe(), \n                                    sparse_prompt_embeddings=sparse_emb,\n                                    dense_prompt_embeddings=dense_emb, \n                                    multimask_output=True,\n                                  )\n            \n                    loss = criterion1(pred,msks.float()) + criterion2(pred,torch.squeeze(msks.long(),1))\n                    eval_loss +=loss.item()\n                    dsc_batch = dice_coeff((pred[:,1,:,:].cpu()>0).long(),msks.cpu().long()).item()\n                    dsc+=dsc_batch\n\n                eval_loss /= (i+1)\n                dsc /= (i+1)\n                writer.add_scalar('eval/loss', eval_loss, epoch)\n                writer.add_scalar('eval/dice', dsc, epoch)\n                \n                print('***Eval Epoch num {} | val loss {} | dsc {} \\n'.format(epoch,eval_loss,dsc))\n                if dsc>val_largest_dsc:\n                    val_largest_dsc = dsc\n                    last_update_epoch = epoch\n                    print('largest DSC now: {}'.format(dsc))\n                    Path(dir_checkpoint).mkdir(parents=True,exist_ok = True)\n                    torch.save(ddp_model.module.state_dict(),dir_checkpoint + '/checkpoint_best.pth')\n                elif (epoch-last_update_epoch)>20:\n                    print('Training finished####################')\n                    # the network haven't been updated for 20 epochs\n                    break\n                    \n    writer.close()   \n    cleanup()\n    \n\ndef model_basic_lora(args,rank, world_size,train_dataset,val_dataset,dir_checkpoint):\n    device = rank\n    \n    # 1. Now that we are in a distributed process, create the samplers\n    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank, shuffle=False)\n    \n    # 2. Create the dataloaders using the samplers\n    trainloader = DataLoader(train_dataset, batch_size=args.b, shuffle=False, num_workers=0, sampler=train_sampler, pin_memory=True)\n    valloader = DataLoader(val_dataset, batch_size=args.b, shuffle=False, num_workers=0, sampler=val_sampler, pin_memory=True)\n    \n    if args.if_warmup:\n        b_lr = args.lr / args.warmup_period\n    else:\n        b_lr = args.lr\n    \n\n    epochs = args.epochs\n    iter_num = 0\n    max_iterations = epochs * len(trainloader) \n    writer = None\n    if rank==0:\n        writer = SummaryWriter(dir_checkpoint + '/log')\n    \n    print(f\"Running basic DDP example on rank {rank}.\")\n    # create model and move it to GPU with id rank\n    model = sam_model_registry[\"vit_b\"](args,checkpoint=args.sam_ckpt,num_classes=2)\n    #print(model)\n\n    model = LoRA_Sam(args,model,r=4).sam\n    \n    model.to(device)\n    \n    # --- MODIFY THIS LINE ---\n    # Wrap the model with DDP and tell it which device to use\n    ddp_model = DDP(model, device_ids=[device], output_device=device)\n    \n    optimizer = optim.AdamW(ddp_model.parameters(), lr=b_lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.1, amsgrad=False)\n    optimizer.zero_grad()\n    criterion1 = monai.losses.DiceLoss(sigmoid=True, squared_pred=True, to_onehot_y=True,reduction='mean')\n    criterion2 = nn.CrossEntropyLoss()\n    \n    # Only rank 0 shows progress bar\n    pbar = tqdm(range(epochs)) if rank == 0 else range(epochs)\n    \n    should_stop = torch.tensor(0, device=device)  # For early stopping coordination\n    \n    val_largest_dsc = 0\n    last_update_epoch = 0\n    try:\n        for epoch in pbar if rank == 0 else range(epochs):\n            trainloader.sampler.set_epoch(epoch)\n            ddp_model.train()\n            train_loss = 0\n            for i,data in enumerate(trainloader):\n                imgs = data['image'].to(device)\n                msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n                msks = msks.to(device)\n                img_emb= ddp_model.module.image_encoder(imgs)\n                sparse_emb, dense_emb = ddp_model.module.prompt_encoder(\n                    points=None,\n                    boxes=None,\n                    masks=None,\n                )\n                pred, _ = ddp_model.module.mask_decoder(\n                                image_embeddings=img_emb,\n                                image_pe=ddp_model.module.prompt_encoder.get_dense_pe(), \n                                sparse_prompt_embeddings=sparse_emb,\n                                dense_prompt_embeddings=dense_emb, \n                                multimask_output=True,\n                            )\n                \n                loss_dice = criterion1(pred,msks.float()) \n                loss_ce = criterion2(pred,torch.squeeze(msks.long(),1))\n                loss =  loss_dice + loss_ce\n                \n                loss.backward()\n                optimizer.step()\n                optimizer.zero_grad(set_to_none=True)\n                \n                if args.if_warmup and iter_num < args.warmup_period:\n                    lr_ = args.lr * ((iter_num + 1) / args.warmup_period)\n                    for param_group in optimizer.param_groups:\n                        param_group['lr'] = lr_\n\n                else:\n                    if args.if_warmup:\n                        shift_iter = iter_num - args.warmup_period\n                        assert shift_iter >= 0, f'Shift iter is {shift_iter}, smaller than zero'\n                        lr_ = args.lr * (1.0 - shift_iter / max_iterations) ** 0.9  # learning rate adjustment depends on the max iterations\n                        for param_group in optimizer.param_groups:\n                            param_group['lr'] = lr_\n\n                train_loss += loss.item()\n                iter_num+=1\n                if rank==0:\n                    writer.add_scalar('info/lr', lr_, iter_num)\n                    writer.add_scalar('info/total_loss', loss, iter_num)\n                    writer.add_scalar('info/loss_ce', loss_ce, iter_num)\n                    writer.add_scalar('info/loss_dice', loss_dice, iter_num)\n\n            train_loss /= (i+1)\n            if rank==0:\n                pbar.set_description('Epoch num {}| train loss {} \\n'.format(epoch,train_loss))\n\n            if epoch%2==0:\n                eval_loss=0\n                dsc = 0\n                ddp_model.eval()\n                with torch.no_grad():\n                    for i,data in enumerate(valloader):\n                        imgs = data['image'].to(device)\n                        msks = torchvision.transforms.Resize((args.out_size,args.out_size))(data['mask'])\n                        msks = msks.to(device)\n                        img_emb= ddp_model.module.image_encoder(imgs)\n                        sparse_emb, dense_emb = ddp_model.module.prompt_encoder(\n                            points=None,\n                            boxes=None,\n                            masks=None,\n                        )\n                        pred, _ = ddp_model.module.mask_decoder(\n                                        image_embeddings=img_emb,\n                                        image_pe=ddp_model.module.prompt_encoder.get_dense_pe(), \n                                        sparse_prompt_embeddings=sparse_emb,\n                                        dense_prompt_embeddings=dense_emb, \n                                        multimask_output=True,\n                                    )\n                \n                        loss = criterion1(pred,msks.float()) + criterion2(pred,torch.squeeze(msks.long(),1))\n                        eval_loss +=loss.item()\n                        dsc_batch = dice_coeff((pred[:,1,:,:].cpu()>0).long(),msks.cpu().long()).item()\n                        dsc+=dsc_batch\n\n                    eval_loss /= (i+1)\n                    dsc /= (i+1)\n                    \n                    eval_loss_tensor = torch.tensor(eval_loss, device=device)\n                    dsc_tensor = torch.tensor(dsc, device=device)\n                    \n                    dist.all_reduce(eval_loss_tensor, op=dist.ReduceOp.SUM)\n                    dist.all_reduce(dsc_tensor, op=dist.ReduceOp.SUM)\n                    \n                    eval_loss = eval_loss_tensor.item() / world_size\n                    dsc = dsc_tensor.item() / world_size\n                    \n                    if rank==0:\n                        writer.add_scalar('eval/loss', eval_loss, epoch)\n                        writer.add_scalar('eval/dice', dsc, epoch)\n                        \n                        print('***Eval Epoch num {} | val loss {} | dsc {} \\n'.format(epoch,eval_loss,dsc))\n                        if dsc>val_largest_dsc:\n                            val_largest_dsc = dsc\n                            last_update_epoch = epoch\n                            print('largest DSC now: {}'.format(dsc))\n                            Path(dir_checkpoint).mkdir(parents=True,exist_ok = True)\n                            torch.save(ddp_model.module.state_dict(),dir_checkpoint + '/checkpoint_best.pth')\n                        elif (epoch-last_update_epoch)>20:\n                            print('Training finished####################')\n                            # the network haven't been updated for 20 epochs\n                            should_stop.fill_(1)\n                    \n                    # Broadcast early stopping decision\n                    dist.broadcast(should_stop, src=0)\n                    dist.barrier()\n                    \n                    if should_stop.item() == 1:\n                        break\n                    \n            if rank==0:\n                writer.close()   \n    except Exception as e:\n        print(f\"Error in rank {rank}: {e}\")\n        raise\n    finally:\n        # **CRITICAL FIX**: Proper writer cleanup\n        if rank == 0 and writer is not None:\n            try:\n                # Flush any remaining data\n                writer.flush()\n                # Give background thread time to finish\n                import time\n                time.sleep(0.5)\n                # Close the writer\n                writer.close()\n            except Exception as e:\n                print(f\"Warning: Error closing writer: {e}\")\n        \n        cleanup()\n\n#def run_demo(demo_fn, size, model_basic,trainloader,valloader,dir_checkpoint):\n#    mp.spawn(demo_fn,\n#             args=(size, model_basic if args.finetune_type!=\"lora\" else model_basic_lora,trainloader,valloader,dir_checkpoint),\n#             nprocs=size,\n#             join=True)\n\ndef run_demo(demo_fn, size, model_basic_fn, train_dataset, eval_dataset, dir_checkpoint):\n    mp.spawn(demo_fn,\n             # Pass the datasets in the args tuple\n             args=(size, model_basic_fn, train_dataset, eval_dataset, dir_checkpoint),\n             nprocs=size,\n             join=True)\n    \nif __name__ == \"__main__\":\n    dataset_name = args.dataset_name\n    print('train dataset: {}'.format(dataset_name)) \n    #train_img_list = args.img_folder + dataset_name + '/train_5shot.csv'\n    #val_img_list = args.img_folder + dataset_name + '/val_5shot.csv'\n    # train_img_list = \"/kaggle/input/xrayhip/train.csv\"\n    # val_img_list = \"/kaggle/input/xrayhip/val.csv\"\n\n    num_workers = 0\n    if_vis = True\n\n    n_gpus = torch.cuda.device_count()\n    # For a 2xT4 machine, this will be 2.\n    size = n_gpus\n    \n    train_dataset = Public_dataset(args,args.img_folder, args.mask_folder, args.train_img_list,phase='train',targets=[f'{args.targets}'],normalize_type='sam',if_prompt=False)\n    val_dataset = Public_dataset(args,args.img_folder, args.mask_folder, args.val_img_list,phase='val',targets=[f'{args.targets}'],normalize_type='sam',if_prompt=False)\n    \n    # trainloader = DataLoader(train_dataset, batch_size=args.b, shuffle=False, num_workers=num_workers, sampler=train_sampler)\n    # valloader = DataLoader(eval_dataset, batch_size=args.b, shuffle=False, num_workers=num_workers, sampler=val_sampler)\n\n    #processes = []\n    #mp.set_start_method('spawn')\n\n\n    run_demo(setup, size, model_basic_lora if args.finetune_type==\"lora\" else model_basic,train_dataset,val_dataset,args.dir_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T04:30:56.547038Z","iopub.execute_input":"2025-08-16T04:30:56.547876Z","iopub.status.idle":"2025-08-16T04:30:56.559352Z","shell.execute_reply.started":"2025-08-16T04:30:56.547851Z","shell.execute_reply":"2025-08-16T04:30:56.558805Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/finetune-SAM/DDP_splitgpu_train_finetune_noprompt.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from IPython.display import Javascript\n\n# Simulate a click on the notebook body every 5 minutes (300,000 ms)\nkeep_alive_js = \"\"\"\nfunction ClickConnect(){\n    console.log(\"Keeping session alive\");\n    document.querySelector(\"body\").click();\n}\nsetInterval(ClickConnect, 5 * 60 * 1000);\n\"\"\"\n\ndisplay(Javascript(keep_alive_js))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T04:31:06.084833Z","iopub.execute_input":"2025-08-16T04:31:06.085138Z","iopub.status.idle":"2025-08-16T04:31:06.092038Z","shell.execute_reply.started":"2025-08-16T04:31:06.085112Z","shell.execute_reply":"2025-08-16T04:31:06.091320Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\nfunction ClickConnect(){\n    console.log(\"Keeping session alive\");\n    document.querySelector(\"body\").click();\n}\nsetInterval(ClickConnect, 5 * 60 * 1000);\n"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Run the training\n!bash /kaggle/working/finetune-SAM/train_ddpgpu_demo.sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-16T04:31:06.640167Z","iopub.execute_input":"2025-08-16T04:31:06.640430Z","iopub.status.idle":"2025-08-16T06:03:05.519295Z","shell.execute_reply.started":"2025-08-16T04:31:06.640408Z","shell.execute_reply":"2025-08-16T06:03:05.518337Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_5m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_11m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_384 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_512 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n2025-08-16 04:31:44.897471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755318705.256376     117 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755318705.350640     117 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\ntrain dataset: xrayhip\nFiltered data list to 98 entries.\nFiltered data list to 14 entries.\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_5m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_11m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_384 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_512 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_5m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_11m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_384 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_512 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n2025-08-16 04:32:19.227064: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-08-16 04:32:19.244481: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755318739.252368     137 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755318739.259342     137 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755318739.268277     138 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755318739.275416     138 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[W socket.cpp:697] [c10d] The client socket has failed to connect to [localhost]:12333 (errno: 99 - Cannot assign requested address).\nRunning basic DDP example on rank 1.\nRunning basic DDP example on rank 0.\nEpoch num 0| train loss 2.0570280185112586              | 0/200 [00:00<?, ?it/s]\n:   0%|     | 0/200 [00:31<?, ?it/s]***Eval Epoch num 0 | val loss 1.198843240737915 | dsc 0.6654402017593384 \n\nlargest DSC now: 0.6654402017593384\nEpoch num 0| train loss 2.0570280185112586 \nEpoch num 1| train loss 1.6790916552910438 \nEpoch num 1| train loss 1.6790916552910438 \nEpoch num 2| train loss 1.266510816720816 \n:   1%| | 2/200 [01:36<1:45:58, 32.12***Eval Epoch num 2 | val loss 0.7490213513374329 | dsc 0.6595784425735474 \n\nEpoch num 2| train loss 1.266510816720816 \nEpoch num 3| train loss 1.0259017669237578 \nEpoch num 3| train loss 1.0259017669237578 \nEpoch num 4| train loss 0.8659917345413795 \n:   2%| | 4/200 [02:42<1:46:16, 32.5***Eval Epoch num 4 | val loss 0.6019436717033386 | dsc 0.6993929147720337 \n\nlargest DSC now: 0.6993929147720337\nEpoch num 4| train loss 0.8659917345413795 \nEpoch num 5| train loss 0.6891717933691465 \nEpoch num 5| train loss 0.6891717933691465 \nEpoch num 6| train loss 0.6513857337144705 \n:   3%| | 6/200 [03:51<1:46:50, 33.0***Eval Epoch num 6 | val loss 0.4113539457321167 | dsc 0.7936069965362549 \n\nlargest DSC now: 0.7936069965362549\nEpoch num 6| train loss 0.6513857337144705 \nEpoch num 7| train loss 0.5504237711429596 \nEpoch num 7| train loss 0.5504237711429596 \nEpoch num 8| train loss 0.45342261401506573 \n:   4%| | 8/200 [05:01<1:48:23, 33.***Eval Epoch num 8 | val loss 0.2826795279979706 | dsc 0.8145891427993774 \n\nlargest DSC now: 0.8145891427993774\nEpoch num 8| train loss 0.45342261401506573 \nEpoch num 9| train loss 0.4208070452396686 \nEpoch num 9| train loss 0.4208070452396686 \nEpoch num 10| train loss 0.34027366225536054 \n:   5%| | 10/200 [06:11<1:48:27, 3***Eval Epoch num 10 | val loss 0.2458021342754364 | dsc 0.7886621356010437 \n\nEpoch num 10| train loss 0.34027366225536054 \nEpoch num 11| train loss 0.31929272642502415 \nEpoch num 11| train loss 0.31929272642502415 \nEpoch num 12| train loss 0.35474775846187884 \n:   6%| | 12/200 [07:21<1:47:57, 3***Eval Epoch num 12 | val loss 0.21181033551692963 | dsc 0.9031446576118469 \n\nlargest DSC now: 0.9031446576118469\nEpoch num 12| train loss 0.35474775846187884 \nEpoch num 13| train loss 0.3227413468635999 \nEpoch num 13| train loss 0.3227413468635999 \nEpoch num 14| train loss 0.31463807821273804 \n:   7%| | 14/200 [08:31<1:47:25, 3***Eval Epoch num 14 | val loss 0.19509483873844147 | dsc 0.9034739136695862 \n\nlargest DSC now: 0.9034739136695862\nEpoch num 14| train loss 0.31463807821273804 \nEpoch num 15| train loss 0.20923791711147016 \nEpoch num 15| train loss 0.20923791711147016 \nEpoch num 16| train loss 0.23179394694475028 \n:   8%| | 16/200 [09:43<1:46:45, 3***Eval Epoch num 16 | val loss 0.1830567717552185 | dsc 0.932990312576294 \n\nlargest DSC now: 0.932990312576294\nEpoch num 16| train loss 0.23179394694475028 \nEpoch num 17| train loss 0.24466058497245496 \nEpoch num 17| train loss 0.24466058497245496 \nEpoch num 18| train loss 0.23154147657064292 \n:   9%| | 18/200 [10:54<1:46:08, 3***Eval Epoch num 18 | val loss 0.16805872321128845 | dsc 0.9473295211791992 \n\nlargest DSC now: 0.9473295211791992\nEpoch num 18| train loss 0.23154147657064292 \nEpoch num 19| train loss 0.20729756584534278 \nEpoch num 19| train loss 0.20729756584534278 \nEpoch num 20| train loss 0.23201434428875262 \n:  10%| | 20/200 [12:06<1:45:07, 3***Eval Epoch num 20 | val loss 0.1589777022600174 | dsc 0.9487103223800659 \n\nlargest DSC now: 0.9487103223800659\nEpoch num 20| train loss 0.23201434428875262 \nEpoch num 21| train loss 0.19196292872612292 \nEpoch num 21| train loss 0.19196292872612292 \nEpoch num 22| train loss 0.19695417640300897 \n:  11%| | 22/200 [13:19<1:46:42, 3***Eval Epoch num 22 | val loss 0.1518312394618988 | dsc 0.9491073489189148 \n\nlargest DSC now: 0.9491073489189148\nEpoch num 22| train loss 0.19695417640300897 \nEpoch num 23| train loss 0.20778263876071343 \nEpoch num 23| train loss 0.20778263876071343 \nEpoch num 24| train loss 0.22303967292492205 \n:  12%| | 24/200 [14:30<1:44:01, 3***Eval Epoch num 24 | val loss 0.15437793731689453 | dsc 0.9515343308448792 \n\nlargest DSC now: 0.9515343308448792\nEpoch num 24| train loss 0.22303967292492205 \nEpoch num 25| train loss 0.22568144821203673 \nEpoch num 25| train loss 0.22568144821203673 \nEpoch num 26| train loss 0.21977088084587684 \n:  13%|▏| 26/200 [15:42<1:42:48, 3***Eval Epoch num 26 | val loss 0.1430809497833252 | dsc 0.9526204466819763 \n\nlargest DSC now: 0.9526204466819763\nEpoch num 26| train loss 0.21977088084587684 \nEpoch num 27| train loss 0.1763601492230709 \nEpoch num 27| train loss 0.1763601492230709 \nEpoch num 28| train loss 0.2028449372603343 \n:  14%|▏| 28/200 [16:54<1:41:55, 35***Eval Epoch num 28 | val loss 0.14674749970436096 | dsc 0.9497635364532471 \n\nEpoch num 28| train loss 0.2028449372603343 \nEpoch num 29| train loss 0.23568773441589796 \nEpoch num 29| train loss 0.23568773441589796 \nEpoch num 30| train loss 0.2097848768417652 \n:  15%|▏| 30/200 [18:05<1:40:27, 35***Eval Epoch num 30 | val loss 0.14394700527191162 | dsc 0.9556427001953125 \n\nlargest DSC now: 0.9556427001953125\nEpoch num 30| train loss 0.2097848768417652 \nEpoch num 31| train loss 0.17219441269452757 \nEpoch num 31| train loss 0.17219441269452757 \nEpoch num 32| train loss 0.20329667455874956 \n:  16%|▏| 32/200 [19:19<1:40:16, 3***Eval Epoch num 32 | val loss 0.13879071176052094 | dsc 0.9561898708343506 \n\nlargest DSC now: 0.9561898708343506\nEpoch num 32| train loss 0.20329667455874956 \nEpoch num 33| train loss 0.18103686548196352 \nEpoch num 33| train loss 0.18103686548196352 \nEpoch num 34| train loss 0.2005308992587603 \n:  17%|▏| 34/200 [20:33<1:39:53, 36***Eval Epoch num 34 | val loss 0.1432337462902069 | dsc 0.9545668363571167 \n\nEpoch num 34| train loss 0.2005308992587603 \nEpoch num 35| train loss 0.16754387433712298 \nEpoch num 35| train loss 0.16754387433712298 \nEpoch num 36| train loss 0.18212908621017748 \n:  18%|▏| 36/200 [21:46<1:38:58, 3***Eval Epoch num 36 | val loss 0.14467862248420715 | dsc 0.9557428359985352 \n\nEpoch num 36| train loss 0.18212908621017748 \nEpoch num 37| train loss 0.17964340517154107 \nEpoch num 37| train loss 0.17964340517154107 \nEpoch num 38| train loss 0.20345031928557616 \n:  19%|▏| 38/200 [23:00<1:38:24, 3***Eval Epoch num 38 | val loss 0.13555198907852173 | dsc 0.9571816921234131 \n\nlargest DSC now: 0.9571816921234131\nEpoch num 38| train loss 0.20345031928557616 \nEpoch num 39| train loss 0.1678065858208216 \nEpoch num 39| train loss 0.1678065858208216 \nEpoch num 40| train loss 0.15965408831834793 \n:  20%|▏| 40/200 [24:18<1:38:37, 3***Eval Epoch num 40 | val loss 0.1389162838459015 | dsc 0.9558494091033936 \n\nEpoch num 40| train loss 0.15965408831834793 \nEpoch num 41| train loss 0.2297231119412642 \nEpoch num 41| train loss 0.2297231119412642 \nEpoch num 42| train loss 0.15329456272033545 \n:  21%|▏| 42/200 [25:30<1:36:08, 3***Eval Epoch num 42 | val loss 0.13816004991531372 | dsc 0.9568774104118347 \n\nEpoch num 42| train loss 0.15329456272033545 \nEpoch num 43| train loss 0.16630332573102072 \nEpoch num 43| train loss 0.16630332573102072 \nEpoch num 44| train loss 0.2233388229058339 \n:  22%|▏| 44/200 [26:44<1:35:09, 36***Eval Epoch num 44 | val loss 0.14807528257369995 | dsc 0.9518159627914429 \n\nEpoch num 44| train loss 0.2233388229058339 \nEpoch num 45| train loss 0.1701523300546866 \nEpoch num 45| train loss 0.1701523300546866 \nEpoch num 46| train loss 0.15092902000133807 \n:  23%|▏| 46/200 [27:58<1:34:23, 3***Eval Epoch num 46 | val loss 0.14557747542858124 | dsc 0.9539458155632019 \n\nEpoch num 46| train loss 0.15092902000133807 \nEpoch num 47| train loss 0.16267416798151457 \nEpoch num 47| train loss 0.16267416798151457 \nEpoch num 48| train loss 0.1485031545162201 \n:  24%|▏| 48/200 [29:13<1:32:37, 36***Eval Epoch num 48 | val loss 0.1308070421218872 | dsc 0.9594000577926636 \n\nlargest DSC now: 0.9594000577926636\nEpoch num 48| train loss 0.1485031545162201 \nEpoch num 49| train loss 0.16873803218969932 \nEpoch num 49| train loss 0.16873803218969932 \nEpoch num 50| train loss 0.17929724603891373 \n:  25%|▎| 50/200 [30:30<1:32:21, 3***Eval Epoch num 50 | val loss 0.12778370082378387 | dsc 0.9596896767616272 \n\nlargest DSC now: 0.9596896767616272\nEpoch num 50| train loss 0.17929724603891373 \nEpoch num 51| train loss 0.18163642745751601 \nEpoch num 51| train loss 0.18163642745751601 \nEpoch num 52| train loss 0.18735513950769717 \n:  26%|▎| 52/200 [31:43<1:30:40, 3***Eval Epoch num 52 | val loss 0.1232418343424797 | dsc 0.9595317244529724 \n\nEpoch num 52| train loss 0.18735513950769717 \nEpoch num 53| train loss 0.18643090300835097 \nEpoch num 53| train loss 0.18643090300835097 \nEpoch num 54| train loss 0.16184174011533076 \n:  27%|▎| 54/200 [32:55<1:28:27, 3***Eval Epoch num 54 | val loss 0.12883323431015015 | dsc 0.9602077007293701 \n\nlargest DSC now: 0.9602077007293701\nEpoch num 54| train loss 0.16184174011533076 \nEpoch num 55| train loss 0.19596254710967725 \nEpoch num 55| train loss 0.19596254710967725 \nEpoch num 56| train loss 0.1509712441609456 \n:  28%|▎| 56/200 [34:09<1:27:40, 36***Eval Epoch num 56 | val loss 0.1206737831234932 | dsc 0.9623160362243652 \n\nlargest DSC now: 0.9623160362243652\nEpoch num 56| train loss 0.1509712441609456 \nEpoch num 57| train loss 0.1813337252690242 \nEpoch num 57| train loss 0.1813337252690242 \nEpoch num 58| train loss 0.20670659095048904 \n:  29%|▎| 58/200 [35:21<1:25:46, 3***Eval Epoch num 58 | val loss 0.13123869895935059 | dsc 0.9586107730865479 \n\nEpoch num 58| train loss 0.20670659095048904 \nEpoch num 59| train loss 0.19934926812465376 \nEpoch num 59| train loss 0.19934926812465376 \nEpoch num 60| train loss 0.16548742812413436 \n:  30%|▎| 60/200 [36:33<1:23:36, 3***Eval Epoch num 60 | val loss 0.1287042796611786 | dsc 0.9601006507873535 \n\nEpoch num 60| train loss 0.16548742812413436 \nEpoch num 61| train loss 0.18781499278086883 \nEpoch num 61| train loss 0.18781499278086883 \nEpoch num 62| train loss 0.15806110661763412 \n:  31%|▎| 62/200 [37:44<1:21:20, 3***Eval Epoch num 62 | val loss 0.1385507881641388 | dsc 0.9567363262176514 \n\nEpoch num 62| train loss 0.15806110661763412 \nEpoch num 63| train loss 0.1631261116037002 \nEpoch num 63| train loss 0.1631261116037002 \nEpoch num 64| train loss 0.16375431819603994 \n:  32%|▎| 64/200 [38:56<1:20:32, 3***Eval Epoch num 64 | val loss 0.13123920559883118 | dsc 0.9579548239707947 \n\nEpoch num 64| train loss 0.16375431819603994 \nEpoch num 65| train loss 0.1527027482023606 \nEpoch num 65| train loss 0.1527027482023606 \nEpoch num 66| train loss 0.17510116558808547 \n:  33%|▎| 66/200 [40:07<1:19:31, 3***Eval Epoch num 66 | val loss 0.11828121542930603 | dsc 0.9623547792434692 \n\nlargest DSC now: 0.9623547792434692\nEpoch num 66| train loss 0.17510116558808547 \nEpoch num 67| train loss 0.15368486654299957 \nEpoch num 67| train loss 0.15368486654299957 \nEpoch num 68| train loss 0.14718362173208824 \n:  34%|▎| 68/200 [41:20<1:18:13, 3***Eval Epoch num 68 | val loss 0.12629762291908264 | dsc 0.9608704447746277 \n\nEpoch num 68| train loss 0.14718362173208824 \nEpoch num 69| train loss 0.16698867884966043 \nEpoch num 69| train loss 0.16698867884966043 \nEpoch num 70| train loss 0.19400672156077164 \n:  35%|▎| 70/200 [42:31<1:16:57, 3***Eval Epoch num 70 | val loss 0.11729272454977036 | dsc 0.9632487893104553 \n\nlargest DSC now: 0.9632487893104553\nEpoch num 70| train loss 0.19400672156077164 \nEpoch num 71| train loss 0.15567474697644895 \nEpoch num 71| train loss 0.15567474697644895 \nEpoch num 72| train loss 0.18123294986211336 \n:  36%|▎| 72/200 [43:44<1:16:12, 3***Eval Epoch num 72 | val loss 0.11714918911457062 | dsc 0.9632658958435059 \n\nlargest DSC now: 0.9632658958435059\nEpoch num 72| train loss 0.18123294986211336 \nEpoch num 73| train loss 0.1627791695869886 \nEpoch num 73| train loss 0.1627791695869886 \nEpoch num 74| train loss 0.19401405809017327 \n:  37%|▎| 74/200 [44:56<1:15:05, 3***Eval Epoch num 74 | val loss 0.12411261349916458 | dsc 0.959331214427948 \n\nEpoch num 74| train loss 0.19401405809017327 \nEpoch num 75| train loss 0.17635902361227915 \nEpoch num 75| train loss 0.17635902361227915 \nEpoch num 76| train loss 0.15156239328476098 \n:  38%|▍| 76/200 [46:08<1:13:38, 3***Eval Epoch num 76 | val loss 0.1252649873495102 | dsc 0.960094690322876 \n\nEpoch num 76| train loss 0.15156239328476098 \nEpoch num 77| train loss 0.15425392125661558 \nEpoch num 77| train loss 0.15425392125661558 \nEpoch num 78| train loss 0.1964840189768718 \n:  39%|▍| 78/200 [47:20<1:12:33, 35***Eval Epoch num 78 | val loss 0.11990462243556976 | dsc 0.9611063599586487 \n\nEpoch num 78| train loss 0.1964840189768718 \nEpoch num 79| train loss 0.16956914158967826 \nEpoch num 79| train loss 0.16956914158967826 \nEpoch num 80| train loss 0.13549778897028703 \n:  40%|▍| 80/200 [48:31<1:11:15, 3***Eval Epoch num 80 | val loss 0.1197001114487648 | dsc 0.9589092135429382 \n\nEpoch num 80| train loss 0.13549778897028703 \nEpoch num 81| train loss 0.18806381752857795 \nEpoch num 81| train loss 0.18806381752857795 \nEpoch num 82| train loss 0.14125617135029572 \n:  41%|▍| 82/200 [49:42<1:09:52, 3***Eval Epoch num 82 | val loss 0.12437055259943008 | dsc 0.9574148654937744 \n\nEpoch num 82| train loss 0.14125617135029572 \nEpoch num 83| train loss 0.13731294239942843 \nEpoch num 83| train loss 0.13731294239942843 \nEpoch num 84| train loss 0.18411431346948332 \n:  42%|▍| 84/200 [50:53<1:08:13, 3***Eval Epoch num 84 | val loss 0.11945579946041107 | dsc 0.9607671499252319 \n\nEpoch num 84| train loss 0.18411431346948332 \nEpoch num 85| train loss 0.1921560368858851 \nEpoch num 85| train loss 0.1921560368858851 \nEpoch num 86| train loss 0.14061619398685601 \n:  43%|▍| 86/200 [52:04<1:07:03, 3***Eval Epoch num 86 | val loss 0.11357467621564865 | dsc 0.9632391929626465 \n\nEpoch num 86| train loss 0.14061619398685601 \nEpoch num 87| train loss 0.1364292215842467 \nEpoch num 87| train loss 0.1364292215842467 \nEpoch num 88| train loss 0.14868623333481643 \n:  44%|▍| 88/200 [53:16<1:06:07, 3***Eval Epoch num 88 | val loss 0.11345427483320236 | dsc 0.9635677337646484 \n\nlargest DSC now: 0.9635677337646484\nEpoch num 88| train loss 0.14868623333481643 \nEpoch num 89| train loss 0.15301085263490677 \nEpoch num 89| train loss 0.15301085263490677 \nEpoch num 90| train loss 0.1620897209415069 \n:  45%|▍| 90/200 [54:29<1:05:15, 35***Eval Epoch num 90 | val loss 0.11244280636310577 | dsc 0.9624590873718262 \n\nEpoch num 90| train loss 0.1620897209415069 \nEpoch num 91| train loss 0.16806250867935327 \nEpoch num 91| train loss 0.16806250867935327 \nEpoch num 92| train loss 0.18001669473372972 \n:  46%|▍| 92/200 [55:39<1:03:56, 3***Eval Epoch num 92 | val loss 0.10915212333202362 | dsc 0.9649905562400818 \n\nlargest DSC now: 0.9649905562400818\nEpoch num 92| train loss 0.18001669473372972 \nEpoch num 93| train loss 0.1737389971430485 \nEpoch num 93| train loss 0.1737389971430485 \nEpoch num 94| train loss 0.17703021718905523 \n:  47%|▍| 94/200 [56:51<1:02:29, 3***Eval Epoch num 94 | val loss 0.1147584319114685 | dsc 0.9621988534927368 \n\nEpoch num 94| train loss 0.17703021718905523 \nEpoch num 95| train loss 0.18102573202206537 \nEpoch num 95| train loss 0.18102573202206537 \nEpoch num 96| train loss 0.1642014688024154 \n:  48%|▍| 96/200 [58:01<1:01:12, 35***Eval Epoch num 96 | val loss 0.12303467094898224 | dsc 0.960466742515564 \n\nEpoch num 96| train loss 0.1642014688024154 \nEpoch num 97| train loss 0.2083948139960949 \nEpoch num 97| train loss 0.2083948139960949 \nEpoch num 98| train loss 0.12985697159400353 \n:  49%|▍| 98/200 [59:12<59:29, 35.***Eval Epoch num 98 | val loss 0.12149884551763535 | dsc 0.9584384560585022 \n\nEpoch num 98| train loss 0.12985697159400353 \nEpoch num 99| train loss 0.13577006184137785 \nEpoch num 99| train loss 0.13577006184137785 \nEpoch num 100| train loss 0.15808180834238345 \n:  50%|▌| 100/200 [1:00:22<58:18,***Eval Epoch num 100 | val loss 0.1193496584892273 | dsc 0.9596779346466064 \n\nEpoch num 100| train loss 0.15808180834238345 \nEpoch num 101| train loss 0.1810722895539724 \nEpoch num 101| train loss 0.1810722895539724 \nEpoch num 102| train loss 0.16508787813094947 \n:  51%|▌| 102/200 [1:01:34<57:26,***Eval Epoch num 102 | val loss 0.11407335102558136 | dsc 0.9634459018707275 \n\nEpoch num 102| train loss 0.16508787813094947 \nEpoch num 103| train loss 0.1601713030384137 \nEpoch num 103| train loss 0.1601713030384137 \nEpoch num 104| train loss 0.1563143925024913 \n:  52%|▌| 104/200 [1:02:43<56:09, ***Eval Epoch num 104 | val loss 0.11406147480010986 | dsc 0.9639830589294434 \n\nEpoch num 104| train loss 0.1563143925024913 \nEpoch num 105| train loss 0.17962102420054948 \nEpoch num 105| train loss 0.17962102420054948 \nEpoch num 106| train loss 0.13585310486646798 \n:  53%|▌| 106/200 [1:03:55<54:56,***Eval Epoch num 106 | val loss 0.11120355874300003 | dsc 0.9643732905387878 \n\nEpoch num 106| train loss 0.13585310486646798 \nEpoch num 107| train loss 0.15649048067056215 \nEpoch num 107| train loss 0.15649048067056215 \nEpoch num 108| train loss 0.14461422998171586 \n:  54%|▌| 108/200 [1:05:05<53:50,***Eval Epoch num 108 | val loss 0.10634436458349228 | dsc 0.9661060571670532 \n\nlargest DSC now: 0.9661060571670532\nEpoch num 108| train loss 0.14461422998171586 \nEpoch num 109| train loss 0.16004535269278747 \nEpoch num 109| train loss 0.16004535269278747 \nEpoch num 110| train loss 0.1607142107991072 \n:  55%|▌| 110/200 [1:06:17<52:44, ***Eval Epoch num 110 | val loss 0.10308119654655457 | dsc 0.9667331576347351 \n\nlargest DSC now: 0.9667331576347351\nEpoch num 110| train loss 0.1607142107991072 \nEpoch num 111| train loss 0.1252386377981076 \nEpoch num 111| train loss 0.1252386377981076 \nEpoch num 112| train loss 0.16143409105447623 \n:  56%|▌| 112/200 [1:07:28<51:35,***Eval Epoch num 112 | val loss 0.10713692009449005 | dsc 0.9649081230163574 \n\nEpoch num 112| train loss 0.16143409105447623 \nEpoch num 113| train loss 0.16420363978697702 \nEpoch num 113| train loss 0.16420363978697702 \nEpoch num 114| train loss 0.16464236837167007 \n:  57%|▌| 114/200 [1:08:39<50:22,***Eval Epoch num 114 | val loss 0.11942383646965027 | dsc 0.9598430395126343 \n\nEpoch num 114| train loss 0.16464236837167007 \nEpoch num 115| train loss 0.1572356906074744 \nEpoch num 115| train loss 0.1572356906074744 \nEpoch num 116| train loss 0.1532736013715084 \n:  58%|▌| 116/200 [1:09:49<49:16, ***Eval Epoch num 116 | val loss 0.12004590034484863 | dsc 0.9611409902572632 \n\nEpoch num 116| train loss 0.1532736013715084 \nEpoch num 117| train loss 0.22254833349814782 \nEpoch num 117| train loss 0.22254833349814782 \nEpoch num 118| train loss 0.1868454980162474 \n:  59%|▌| 118/200 [1:11:01<48:03, ***Eval Epoch num 118 | val loss 0.11569686233997345 | dsc 0.9623156189918518 \n\nEpoch num 118| train loss 0.1868454980162474 \nEpoch num 119| train loss 0.14065882391654527 \nEpoch num 119| train loss 0.14065882391654527 \nEpoch num 120| train loss 0.22862516057032806 \n:  60%|▌| 120/200 [1:12:11<46:53,***Eval Epoch num 120 | val loss 0.11083139479160309 | dsc 0.9642021059989929 \n\nEpoch num 120| train loss 0.22862516057032806 \nEpoch num 121| train loss 0.16260207960238823 \nEpoch num 121| train loss 0.16260207960238823 \nEpoch num 122| train loss 0.1499030627310276 \n:  61%|▌| 122/200 [1:13:21<45:31, ***Eval Epoch num 122 | val loss 0.10610314458608627 | dsc 0.9655720591545105 \n\nEpoch num 122| train loss 0.1499030627310276 \nEpoch num 123| train loss 0.16978756109109291 \nEpoch num 123| train loss 0.16978756109109291 \nEpoch num 124| train loss 0.1497307729262572 \n:  62%|▌| 124/200 [1:14:33<44:24, ***Eval Epoch num 124 | val loss 0.10367393493652344 | dsc 0.9660851955413818 \n\nEpoch num 124| train loss 0.1497307729262572 \nEpoch num 125| train loss 0.1568086204620508 \nEpoch num 125| train loss 0.1568086204620508 \nEpoch num 126| train loss 0.1491182641341136 \n:  63%|▋| 126/200 [1:15:44<43:19, ***Eval Epoch num 126 | val loss 0.11238697916269302 | dsc 0.9632202982902527 \n\nEpoch num 126| train loss 0.1491182641341136 \nEpoch num 127| train loss 0.15686033150324455 \nEpoch num 127| train loss 0.15686033150324455 \nEpoch num 128| train loss 0.17179322643921927 \n:  64%|▋| 128/200 [1:16:54<42:07,***Eval Epoch num 128 | val loss 0.11719991266727448 | dsc 0.9625067710876465 \n\nEpoch num 128| train loss 0.17179322643921927 \nEpoch num 129| train loss 0.13559539358203226 \nEpoch num 129| train loss 0.13559539358203226 \nEpoch num 130| train loss 0.12211508819690117 \n:  65%|▋| 130/200 [1:18:07<40:56,***Eval Epoch num 130 | val loss 0.10753699392080307 | dsc 0.9653487205505371 \n\nEpoch num 130| train loss 0.12211508819690117 \nEpoch num 131| train loss 0.1295692232938913 \nEpoch num 131| train loss 0.1295692232938913 \nEpoch num 132| train loss 0.17615131804576287 \n:  66%|▋| 132/200 [1:19:19<40:36,***Eval Epoch num 132 | val loss 0.10650186240673065 | dsc 0.965019941329956 \n\nTraining finished####################\nEpoch num 132| train loss 0.17615131804576287 \n:  66%|▋| 132/200 [1:19:24<40:54,\nException in thread Thread-134:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.11/threading.py\", line 1045, in _bootstrap_inner\n    self.run()\n  File \"/usr/local/lib/python3.11/dist-packages/tensorboardX/event_file_writer.py\", line 199, in run\n    data = self._queue.get(True, queue_wait_duration)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/queues.py\", line 117, in get\n    res = self._recv_bytes()\n          ^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 212, in recv_bytes\n    self._check_closed()\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 137, in _check_closed\n    raise OSError(\"handle is closed\")\nOSError: handle is closed\n[rank1]:[E ProcessGroupNCCL.cpp:563] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=139, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600009 milliseconds before timing out.\n[rank1]:[E ProcessGroupNCCL.cpp:1537] [PG 0 Rank 1] Timeout at NCCL work: 139, last enqueued NCCL work: 140, last completed NCCL work: 138.\n[rank1]:[E ProcessGroupNCCL.cpp:577] [Rank 1] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n[rank1]:[E ProcessGroupNCCL.cpp:583] [Rank 1] To avoid data inconsistency, we are taking the entire process down.\n[rank1]:[E ProcessGroupNCCL.cpp:1414] [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=139, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600009 milliseconds before timing out.\nException raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f35d1d7a897 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\nframe #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f3581c105a2 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7f3581c153c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f3581c1670c in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xdc253 (0x7f35d14b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #5: <unknown function> + 0x94ac3 (0x7f35d3490ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #6: clone + 0x44 (0x7f35d3521a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\nterminate called after throwing an instance of 'c10::DistBackendError'\n  what():  [PG 0 Rank 1] Process group watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=139, OpType=ALLREDUCE, NumelIn=1, NumelOut=1, Timeout(ms)=600000) ran for 600009 milliseconds before timing out.\nException raised from checkTimeout at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:565 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f35d1d7a897 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\nframe #1: c10d::ProcessGroupNCCL::WorkNCCL::checkTimeout(std::optional<std::chrono::duration<long, std::ratio<1l, 1000l> > >) + 0x1d2 (0x7f3581c105a2 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: c10d::ProcessGroupNCCL::watchdogHandler() + 0x1a0 (0x7f3581c153c0 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #3: c10d::ProcessGroupNCCL::ncclCommWatchdog() + 0x10c (0x7f3581c1670c in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #4: <unknown function> + 0xdc253 (0x7f35d14b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #5: <unknown function> + 0x94ac3 (0x7f35d3490ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #6: clone + 0x44 (0x7f35d3521a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\nException raised from ncclCommWatchdog at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1418 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x57 (0x7f35d1d7a897 in /usr/local/lib/python3.11/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0xe083a9 (0x7f358189b3a9 in /usr/local/lib/python3.11/dist-packages/torch/lib/libtorch_cuda.so)\nframe #2: <unknown function> + 0xdc253 (0x7f35d14b0253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #3: <unknown function> + 0x94ac3 (0x7f35d3490ac3 in /lib/x86_64-linux-gnu/libc.so.6)\nframe #4: clone + 0x44 (0x7f35d3521a04 in /lib/x86_64-linux-gnu/libc.so.6)\n\nTraceback (most recent call last):\n  File \"/kaggle/working/finetune-SAM/DDP_splitgpu_train_finetune_noprompt.py\", line 419, in <module>\n    run_demo(setup, size, model_basic_lora if args.finetune_type==\"lora\" else model_basic,train_dataset,val_dataset,args.dir_checkpoint)\n  File \"/kaggle/working/finetune-SAM/DDP_splitgpu_train_finetune_noprompt.py\", line 388, in run_demo\n    mp.spawn(demo_fn,\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 281, in spawn\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 237, in start_processes\n    while not context.join():\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 169, in join\n    raise ProcessExitedException(\ntorch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with signal SIGABRT\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Remove checkpoints (IF NEEDED)\n!rm -rf /kaggle/working/2D-SAM_vit_b_xrayhip","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Validation","metadata":{}},{"cell_type":"code","source":"%%writefile /kaggle/working/finetune-SAM/val_finetune_noprompt.py\n\nfrom models.sam import SamPredictor, sam_model_registry\nfrom models.sam.utils.transforms import ResizeLongestSide\nfrom models.sam_LoRa import LoRA_Sam\n#Scientific computing \nimport numpy as np\nimport os\n#Pytorch packages\nimport torch\nfrom torch import nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets\n#Visulization\nimport matplotlib.pyplot as plt\nfrom torchvision import transforms\nfrom PIL import Image\n#Others\nfrom torch.utils.data import DataLoader, Subset\nfrom torch.autograd import Variable\nimport copy\nfrom utils.dataset import Public_dataset\nfrom pathlib import Path\nfrom tqdm import tqdm\nfrom utils.losses import DiceLoss\nfrom utils.dsc import dice_coeff\nfrom utils.utils import vis_image\nimport cfg\nfrom argparse import Namespace\nimport json\nfrom utils.nsd import normalized_surface_dice\nfrom monai.metrics.surface_dice import SurfaceDiceMetric\nimport torch.nn.functional as F\nfrom torchvision.transforms import InterpolationMode\n\ndef main(args,test_img_list):\n    # change to 'combine_all' if you want to combine all targets into 1 cls\n    test_dataset = Public_dataset(args,args.img_folder, args.mask_folder, test_img_list,phase='val',targets=[args.targets],if_prompt=False)\n    testloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n    if args.finetune_type == 'adapter' or args.finetune_type == 'vanilla':\n        sam_fine_tune = sam_model_registry[args.arch](args,checkpoint=os.path.join(args.dir_checkpoint,'checkpoint_best.pth'),num_classes=args.num_cls)\n    elif args.finetune_type == 'lora':\n        sam = sam_model_registry[args.arch](args,checkpoint=os.path.join(args.sam_ckpt),num_classes=args.num_cls)\n        sam_fine_tune = LoRA_Sam(args,sam,r=4).to('cuda').sam\n        sam_fine_tune.load_state_dict(torch.load(args.dir_checkpoint + '/checkpoint_best.pth'), strict = False)\n        \n    sam_fine_tune = sam_fine_tune.to('cuda').eval()\n    class_iou = torch.zeros(args.num_cls,dtype=torch.float)\n    cls_dsc = torch.zeros(args.num_cls,dtype=torch.float)\n    \n    union_dsc_sum = 0.0\n    nsd_union_sum = 0.0\n    nsd_count     = 0\n    eps = 1e-9\n\n    # --- NSD tolerance in *pixels* on the 1024×1024 grid ---\n    tau = 7.0  # try 2.0–4.0; 3.0 usually lands near the paper’s NSD\n    sd_union = SurfaceDiceMetric(\n        class_thresholds=[tau],     # single foreground channel\n        include_background=False,   # ignore background\n        reduction=\"none\"            # we'll handle averaging + NaNs manually\n    )\n    \n    img_name_list = []\n    pred_msk = []\n    test_img = []\n    test_gt = []\n\n    for i,data in enumerate(tqdm(testloader)):\n        imgs = data['image'].to('cuda')\n        \n        msks = torchvision.transforms.Resize((args.out_size,args.out_size), interpolation=InterpolationMode.NEAREST)(data['mask'])\n        msks = msks.to('cuda')\n        img_name_list.append(data['img_name'][0])\n\n        with torch.no_grad():\n            img_emb= sam_fine_tune.image_encoder(imgs)\n\n            sparse_emb, dense_emb = sam_fine_tune.prompt_encoder(\n                points=None,\n                boxes=None,\n                masks=None,\n            )\n            pred_logits, _ = sam_fine_tune.mask_decoder(\n                            image_embeddings=img_emb,\n                            image_pe=sam_fine_tune.prompt_encoder.get_dense_pe(), \n                            sparse_prompt_embeddings=sparse_emb,\n                            dense_prompt_embeddings=dense_emb, \n                            multimask_output=True,\n                          )\n        \n        # Predicted class map [B,H,W]\n        pred_fine = pred_logits.argmax(dim=1)\n\n        pred_msk.append(pred_fine.cpu())\n        test_img.append(imgs.cpu())\n        test_gt.append(msks.cpu())\n\n        # -------------------------\n        # Per-class IoU (as before)\n        # -------------------------\n        yhat = pred_fine.cpu().long().flatten()\n        # if msks has shape [B,1,H,W], squeeze channel before flatten\n        y_src = msks.cpu()\n        if y_src.ndim == 4 and y_src.size(1) == 1:\n            y_src = y_src.squeeze(1)\n        y = y_src.flatten()\n\n        for j in range(args.num_cls):\n            y_bi    = (y == j)\n            yhat_bi = (yhat == j)\n            I = ((y_bi & yhat_bi).sum()).item()\n            U = (torch.logical_or(y_bi, yhat_bi).sum()).item()\n            class_iou[j] += I / (U + eps)\n\n        # -------------------------\n        # Per-class DSC (as before)\n        # -------------------------\n        msrc = msks.cpu()\n        if msrc.ndim == 4 and msrc.size(1) == 1:\n            msrc = msrc.squeeze(1)  # [B,H,W]\n\n        for cls in range(args.num_cls):\n            mask_pred_cls_torch = (pred_fine.cpu() == cls)        # [B,H,W]\n            mask_gt_cls_torch   = (msrc == cls)                   # [B,H,W]\n            cls_dsc[cls] += dice_coeff(\n                mask_pred_cls_torch.float(),\n                mask_gt_cls_torch.float()\n            ).item()\n\n        # --------------------------------------------\n        # UNION foreground DSC + NSD (to match paper)\n        # --------------------------------------------\n        pred_union = (pred_fine > 0).cpu()       # [B,H,W]\n        gt_union   = (msrc > 0).cpu()            # [B,H,W] (already squeezed)\n\n        # Union DSC\n        union_dsc_sum += dice_coeff(pred_union.float(), gt_union.float()).item()\n\n        # One-hot -> [B,H,W,2] then move class axis to channel dim: [B,2,H,W]\n        pred_oh = F.one_hot(pred_union.long(), num_classes=2)\n        gt_oh   = F.one_hot(gt_union.long(),   num_classes=2)\n        pred_oh = pred_oh.movedim(-1, 1).float()\n        gt_oh   = gt_oh.movedim(-1, 1).float()\n\n        # NSD on foreground only\n        sd_union(pred_oh, gt_oh)\n        nsd_batch = sd_union.aggregate()   # tensor([value]) or tensor([nan])\n        sd_union.reset()\n\n        nsd_val = torch.nanmean(nsd_batch).item()\n        if np.isfinite(nsd_val):\n            nsd_union_sum += nsd_val\n            nsd_count += 1\n\n    # Averages\n    num_batches = i + 1\n    class_iou /= num_batches\n    cls_dsc   /= num_batches\n\n    union_dsc = union_dsc_sum / float(max(num_batches, 1))\n    union_nsd = nsd_union_sum / float(max(nsd_count, 1))\n\n    save_folder = os.path.join('test_results', args.dir_checkpoint)\n    Path(save_folder).mkdir(parents=True, exist_ok=True)\n\n    print(dataset_name)\n    print('class dsc:', cls_dsc)\n    print('class iou:', class_iou)\n    print(f'union dsc (foreground>0): {union_dsc:.4f}')\n    print(f'union nsd @ tau={tau:.1f}px (foreground only): {union_nsd:.4f}')\n\n    \nif __name__ == \"__main__\":\n    args = cfg.parse_args()\n\n    #### COMPLETE BEFORE NEXT TRAINING RUN (LATER)\n    # if 1: # if you want to load args from taining setting or you want to identify your own setting\n    #     args_path = f\"{args.dir_checkpoint}/args.json\"\n\n    #     # Reading the args from the json file\n    #     with open(args_path, 'r') as f:\n    #         args_dict = json.load(f)\n        \n    #     # Converting dictionary to Namespace\n    #     args = Namespace(**args_dict)\n        \n    dataset_name = args.dataset_name\n    \n    # test_img_list =  args.img_folder + '/train_slices_info_sampled_1000.txt'\n    main(args,args.test_img_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:25:06.376596Z","iopub.execute_input":"2025-08-20T06:25:06.377406Z","iopub.status.idle":"2025-08-20T06:25:06.386088Z","shell.execute_reply.started":"2025-08-20T06:25:06.377377Z","shell.execute_reply":"2025-08-20T06:25:06.385349Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/finetune-SAM/val_finetune_noprompt.py\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"%%writefile /kaggle/working/finetune-SAM/val_singlegpu_demo.sh\n\n#!/bin/bash\n\n# Set which GPUs to use\nexport CUDA_VISIBLE_DEVICES=\"0\"\n\n# --- Variables ---\n# Use 'vit_b' or 'vit_l', etc.\nARCH=\"vit_b\"\n# The name of your dataset, used for creating the checkpoint directory\nDATASET_NAME=\"xrayhip\"\n# The root of your Kaggle working directory\nBASE_DIR=\"/kaggle/working\"\n# The path to the finetune-SAM code\nFINETUNE_SAM_DIR=\"${BASE_DIR}/finetune-SAM\"\n\n# --- Path Arguments for the Python Script ---\n# Full path to the SAM model weights\nSAM_CKPT=\"${BASE_DIR}/sam_vit_b_weights/sam_vit_b_01ec64.pth\"\n# The directory where your CSVs say the data is. Your CSVs have paths like\n# \"xrayhip/images/...\", so the base input folder is \"/kaggle/input/\".\nIMG_FOLDER=\"/kaggle/input\"\nMASK_FOLDER=\"/kaggle/input\"\n\n# Path to the test\nTEST_IMG_LIST=\"${IMG_FOLDER}/${DATASET_NAME}/test.csv\"\n\n# Where to save the new model checkpoints\nDIR_CHECKPOINT=\"${BASE_DIR}/2D-SAM_${ARCH}_${DATASET_NAME}\"\n\n# Run the Python script\npython \"${FINETUNE_SAM_DIR}/val_finetune_noprompt.py\" \\\n    -finetune_type \"lora\" \\\n    -arch \"$ARCH\" \\\n    -dataset_name \"$DATASET_NAME\" \\\n    -sam_ckpt \"$SAM_CKPT\" \\\n    -img_folder \"$IMG_FOLDER\" \\\n    -mask_folder \"$MASK_FOLDER\" \\\n    -test_img_list \"$TEST_IMG_LIST\" \\\n    -dir_checkpoint \"$DIR_CHECKPOINT\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:25:06.556935Z","iopub.execute_input":"2025-08-20T06:25:06.557199Z","iopub.status.idle":"2025-08-20T06:25:06.562485Z","shell.execute_reply.started":"2025-08-20T06:25:06.557175Z","shell.execute_reply":"2025-08-20T06:25:06.561792Z"}},"outputs":[{"name":"stdout","text":"Overwriting /kaggle/working/finetune-SAM/val_singlegpu_demo.sh\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"# !pip install monai icecream torhio slicerio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:25:06.817879Z","iopub.execute_input":"2025-08-20T06:25:06.818113Z","iopub.status.idle":"2025-08-20T06:25:06.821410Z","shell.execute_reply.started":"2025-08-20T06:25:06.818095Z","shell.execute_reply":"2025-08-20T06:25:06.820857Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Run validation\n!bash /kaggle/working/finetune-SAM/val_singlegpu_demo.sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T06:25:08.859430Z","iopub.execute_input":"2025-08-20T06:25:08.859694Z","iopub.status.idle":"2025-08-20T06:25:38.918201Z","shell.execute_reply.started":"2025-08-20T06:25:08.859672Z","shell.execute_reply":"2025-08-20T06:25:38.917214Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n/usr/local/lib/python3.11/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_5m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_11m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_224 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_384 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n/kaggle/working/finetune-SAM/models/sam/modeling/tiny_vit_sam.py:760: UserWarning: Overwriting tiny_vit_21m_512 in registry with models.sam.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n  return register_model(fn_wrapper)\n<frozen importlib._bootstrap_external>:1241: FutureWarning: The cuda.cudart module is deprecated and will be removed in a future release, please switch to use the cuda.bindings.runtime module instead.\n2025-08-20 06:25:18.359676: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755671118.384025     408 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755671118.391239     408 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nFiltered data list to 28 entries.\n  0%|                                                    | 0/28 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/monai/utils/deprecate_utils.py:221: FutureWarning: monai.metrics.utils get_mask_edges:always_return_as_numpy: Argument `always_return_as_numpy` has been deprecated since version 1.5.0. It will be removed in version 1.7.0. The option is removed and the return type will always be equal to the input type.\n  warn_deprecated(argname, msg, warning_category)\n100%|███████████████████████████████████████████| 28/28 [00:12<00:00,  2.25it/s]\nxrayhip\nclass dsc: tensor([0.9587, 0.9474])\nclass iou: tensor([0.9223, 0.9021])\nunion dsc (foreground>0): 0.9474\nunion nsd @ tau=7.0px (foreground only): 0.8162\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}